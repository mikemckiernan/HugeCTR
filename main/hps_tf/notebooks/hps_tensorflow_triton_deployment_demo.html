<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Deploy SavedModel using HPS with Triton TensorFlow Backend &mdash; Merlin HugeCTR  documentation</title><link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/mystnb.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/custom.css" type="text/css" />
    <link rel="canonical" href="https://nvidia-merlin.github.io/HugeCTR/main/hps_tf/notebooks/hps_tensorflow_triton_deployment_demo.html" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="../../_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Hierarchical Parameter Server API" href="../../hierarchical_parameter_server/api/index.html" />
    <link rel="prev" title="SOK to HPS DLRM Demo" href="sok_to_hps_dlrm_demo.html" /> 
</head>

<body class="wy-body-for-nav">
  <div class="banner">
    <p class="banner">
      Beginning in January 2023, versions for all NVIDIA Merlin projects
      will change from semantic versioning like <code>4.0</code>
      to calendar versioning like <code>23.01</code>.</p>
  </div>

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">HUGECTR</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hugectr_embedding_training_cache.html">Embedding Training Cache</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../hierarchical_parameter_server/index.html">Hierarchical Parameter Server</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../hugectr_parameter_server.html">HPS Database Backend</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../hierarchical_parameter_server/hps_tf_user_guide.html">HPS Plugin for TensorFlow</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="index.html">Notebooks</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="hierarchical_parameter_server_demo.html">Hierarchical Parameter Server Demo</a></li>
<li class="toctree-l4"><a class="reference internal" href="hps_multi_table_sparse_input_demo.html">HPS for Multiple Tables and Sparse Inputs</a></li>
<li class="toctree-l4"><a class="reference internal" href="hps_pretrained_model_training_demo.html">HPS Pretrained Model Training Demo</a></li>
<li class="toctree-l4"><a class="reference internal" href="sok_to_hps_dlrm_demo.html">SOK to HPS DLRM Demo</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Deploy SavedModel using HPS with Triton TensorFlow Backend</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../hierarchical_parameter_server/api/index.html">API Documentation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse_operation_kit.html">Sparse Operation Kit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release_notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hugectr_contributor_guide.html">Contributing to HugeCTR</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a></li>
          <li class="breadcrumb-item"><a href="../../hierarchical_parameter_server/index.html">Hierarchical Parameter Server</a></li>
          <li class="breadcrumb-item"><a href="../../hierarchical_parameter_server/hps_tf_user_guide.html">Hierarchical Parameter Server Plugin for TensorFlow</a></li>
          <li class="breadcrumb-item"><a href="index.html">Hierarchical Parameter Server Notebooks</a></li>
      <li class="breadcrumb-item active">Deploy SavedModel using HPS with Triton TensorFlow Backend</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <img alt="http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hps-hps-tensorflow-triton-deployment/nvidia_logo.png" src="http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_hps-hps-tensorflow-triton-deployment/nvidia_logo.png" />
<div class="tex2jax_ignore mathjax_ignore section" id="deploy-savedmodel-using-hps-with-triton-tensorflow-backend">
<h1>Deploy SavedModel using HPS with Triton TensorFlow Backend<a class="headerlink" href="#deploy-savedmodel-using-hps-with-triton-tensorflow-backend" title="Permalink to this headline"></a></h1>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline"></a></h2>
<p>This notebook demonstrates how to deploy the SavedModel that leverages HPS with <a class="reference external" href="https://github.com/triton-inference-server/tensorflow_backend">Triton TensorFlow backend</a>. It also shows how to apply <a class="reference external" href="https://github.com/tensorflow/tensorrt">TF-TRT</a> optimization to SavedModel whose embedding lookup is based on HPS. It is recommended to run <a class="reference internal" href="hierarchical_parameter_server_demo.html"><span class="doc std std-doc">hierarchical_parameter_server_demo.ipynb</span></a> before diving into this notebook.</p>
<p>For more details about HPS APIs, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/api/index.html">HPS APIs</a>. For more details about HPS, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/index.html">HugeCTR Hierarchical Parameter Server (HPS)</a>.</p>
</div>
<div class="section" id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline"></a></h2>
<div class="section" id="get-hps-from-ngc">
<h3>Get HPS from NGC<a class="headerlink" href="#get-hps-from-ngc" title="Permalink to this headline"></a></h3>
<p>The HPS Python module is preinstalled in the 22.12 and later <a class="reference external" href="https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow">Merlin TensorFlow Container</a>: <code class="docutils literal notranslate"><span class="pre">nvcr.io/nvidia/merlin/merlin-tensorflow:22.12</span></code>.</p>
<p>You can check the existence of the required libraries by running the following Python code after launching this container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ python3 -c <span class="s2">&quot;import hierarchical_parameter_server as hps&quot;</span>
</pre></div>
</div>
<p>The Triton TensorFlow backend is also available in this container.</p>
</div>
</div>
<div class="section" id="configurations">
<h2>Configurations<a class="headerlink" href="#configurations" title="Permalink to this headline"></a></h2>
<p>First of all we specify the required configurations, e.g., the arguments needed for generating the dataset, the paths to save the model and the model parameters. We will use a deep neural network (DNN) model which has one embedding table and several dense layers in this notebook. Please note that there are two inputs here, one is the key tensor (one-hot) while the other is the dense feature tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">hierarchical_parameter_server</span> <span class="k">as</span> <span class="nn">hps</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">struct</span>

<span class="n">args</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>

<span class="n">args</span><span class="p">[</span><span class="s2">&quot;gpu_num&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>                               <span class="c1"># the number of available GPUs</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;iter_num&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>                             <span class="c1"># the number of training iteration</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;slot_num&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>                              <span class="c1"># the number of feature fields in this embedding layer</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;embed_vec_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">16</span>                       <span class="c1"># the dimension of embedding vectors</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;global_batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1024</span>                  <span class="c1"># the globally batchsize for all GPUs</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;max_vocabulary_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;vocabulary_range_per_slot&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="mi">10000</span><span class="p">],[</span><span class="mi">10000</span><span class="p">,</span><span class="mi">20000</span><span class="p">],[</span><span class="mi">20000</span><span class="p">,</span><span class="mi">30000</span><span class="p">],[</span><span class="mi">30000</span><span class="p">,</span><span class="mi">40000</span><span class="p">],[</span><span class="mi">40000</span><span class="p">,</span><span class="mi">50000</span><span class="p">]]</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;dense_dim&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">10</span>

<span class="n">args</span><span class="p">[</span><span class="s2">&quot;dense_model_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;hps_tf_triton_dense.model&quot;</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;ps_config_file&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;hps_tf_triton.json&quot;</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;embedding_table_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;hps_tf_triton_sparse_0.model&quot;</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;saved_path&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;hps_tf_triton_tf_saved_model&quot;</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;np_key_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;np_vector_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;tf_key_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">int64</span>
<span class="n">args</span><span class="p">[</span><span class="s2">&quot;tf_vector_type&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span>


<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;gpu_num&quot;</span><span class="p">])))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[INFO] hierarchical_parameter_server is imported
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.12) or chardet (3.0.4) doesn&#39;t match a supported version!
  warnings.warn(&quot;urllib3 ({}) or chardet ({}) doesn&#39;t match a supported &quot;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_random_samples</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">vocabulary_range_per_slot</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">,</span> <span class="n">key_dtype</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;np_key_type&quot;</span><span class="p">]):</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">vocab_range</span> <span class="ow">in</span> <span class="n">vocabulary_range_per_slot</span><span class="p">:</span>
        <span class="n">keys_per_slot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="n">vocab_range</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">high</span><span class="o">=</span><span class="n">vocab_range</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">key_dtype</span><span class="p">)</span>
        <span class="n">keys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">keys_per_slot</span><span class="p">)</span>
    <span class="n">keys</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">keys</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">dense_features</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">((</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">low</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">,</span> <span class="n">labels</span>

<span class="k">def</span> <span class="nf">tf_dataset</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">batchsize</span><span class="p">):</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">batchsize</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="train-with-native-tf-layers">
<h2>Train with native TF layers<a class="headerlink" href="#train-with-native-tf-layers" title="Permalink to this headline"></a></h2>
<p>We define the model graph for training with native TF layers, i.e., <code class="docutils literal notranslate"><span class="pre">tf.nn.embedding_lookup</span></code> and <code class="docutils literal notranslate"><span class="pre">tf.keras.layers.Dense</span></code>. Besides, the embedding weights are stored in <code class="docutils literal notranslate"><span class="pre">tf.Variable</span></code>. We can then train the model and extract the trained weights of the embedding table. As for the dense layers, they are saved as a separate model graph, which can be loaded directly during inference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TrainModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">init_tensors</span><span class="p">,</span>
                 <span class="n">slot_num</span><span class="p">,</span>
                 <span class="n">embed_vec_size</span><span class="p">,</span>
                 <span class="n">dense_dim</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TrainModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span> <span class="o">=</span> <span class="n">slot_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_vec_size</span> <span class="o">=</span> <span class="n">embed_vec_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_dim</span> <span class="o">=</span> <span class="n">dense_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_tensors</span> <span class="o">=</span> <span class="n">init_tensors</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">initial_value</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">init_tensors</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">concat</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Concatenate</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;concatenate&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_1</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span>
                                                 <span class="n">bias_initializer</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
                                                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc_1&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                 <span class="n">kernel_initializer</span><span class="o">=</span><span class="s2">&quot;ones&quot;</span><span class="p">,</span>
                                                 <span class="n">bias_initializer</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
                                                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;fc_2&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">ids</span><span class="o">=</span><span class="n">keys</span><span class="p">)</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">embedding_vector</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_vec_size</span><span class="p">])</span>
        <span class="n">concated_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">embedding_vector</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">])</span>
        <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc_1</span><span class="p">(</span><span class="n">concated_features</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">logit</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;tf_key_type&quot;</span><span class="p">]),</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)]</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
    <span class="n">init_tensors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;max_vocabulary_size&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;embed_vec_size&quot;</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;np_vector_type&quot;</span><span class="p">])</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">TrainModel</span><span class="p">(</span><span class="n">init_tensors</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;slot_num&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;embed_vec_size&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;dense_dim&quot;</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">BinaryCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">_train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>
            <span class="n">logit</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">logit</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="n">tape</span><span class="o">.</span><span class="n">gradient</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">logit</span><span class="p">,</span> <span class="n">loss</span>

    <span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">generate_random_samples</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;global_batch_size&quot;</span><span class="p">]</span>  <span class="o">*</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;iter_num&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;vocabulary_range_per_slot&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;dense_dim&quot;</span><span class="p">])</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf_dataset</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;global_batch_size&quot;</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">]</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">_train_step</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">20</span><span class="p">,</span> <span class="s2">&quot;Step </span><span class="si">{}</span><span class="s2">, loss: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">loss</span><span class="p">),</span>  <span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">20</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">trained_model</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
<span class="n">weights_list</span> <span class="o">=</span> <span class="n">trained_model</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span>
<span class="n">embedding_weights</span> <span class="o">=</span> <span class="n">weights_list</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">dense_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">trained_model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;concatenate&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">input</span><span class="p">,</span>
                             <span class="n">trained_model</span><span class="o">.</span><span class="n">get_layer</span><span class="p">(</span><span class="s2">&quot;fc_2&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">output</span><span class="p">)</span>
<span class="n">dense_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
<span class="n">dense_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;dense_model_path&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-11-23 01:36:13.919938: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-11-23 01:36:14.444040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30991 MB memory:  -&gt; device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:The following Variables were used in a Lambda layer&#39;s call (tf.compat.v1.nn.embedding_lookup), but are not present in its tracked objects:   &lt;tf.Variable &#39;Variable:0&#39; shape=(50000, 16) dtype=float32&gt;. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.
Model: &quot;model&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 tf.compat.v1.nn.embedding_look  (None, 5, 16)       0           [&#39;input_1[0][0]&#39;]                
 up (TFOpLambda)                                                                                  
                                                                                                  
 tf.reshape (TFOpLambda)        (None, 80)           0           [&#39;tf.compat.v1.nn.embedding_looku
                                                                 p[0][0]&#39;]                        
                                                                                                  
 input_2 (InputLayer)           [(None, 10)]         0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 90)           0           [&#39;tf.reshape[0][0]&#39;,             
                                                                  &#39;input_2[0][0]&#39;]                
                                                                                                  
 fc_1 (Dense)                   (None, 256)          23296       [&#39;concatenate[0][0]&#39;]            
                                                                                                  
 fc_2 (Dense)                   (None, 1)            257         [&#39;fc_1[0][0]&#39;]                   
                                                                                                  
==================================================================================================
Total params: 23,553
Trainable params: 23,553
Non-trainable params: 0
__________________________________________________________________________________________________
-------------------- Step 0, loss: 10934.333984375 --------------------
-------------------- Step 1, loss: 9218.0703125 --------------------
-------------------- Step 2, loss: 7060.255859375 --------------------
-------------------- Step 3, loss: 5094.876953125 --------------------
-------------------- Step 4, loss: 3605.475830078125 --------------------
-------------------- Step 5, loss: 2593.270751953125 --------------------
-------------------- Step 6, loss: 1741.0677490234375 --------------------
-------------------- Step 7, loss: 1045.5091552734375 --------------------
-------------------- Step 8, loss: 541.4227905273438 --------------------
-------------------- Step 9, loss: 242.8596649169922 --------------------
Model: &quot;model_1&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_3 (InputLayer)           [(None, 80)]         0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10)]         0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 90)           0           [&#39;input_3[0][0]&#39;,                
                                                                  &#39;input_2[0][0]&#39;]                
                                                                                                  
 fc_1 (Dense)                   (None, 256)          23296       [&#39;concatenate[1][0]&#39;]            
                                                                                                  
 fc_2 (Dense)                   (None, 1)            257         [&#39;fc_1[1][0]&#39;]                   
                                                                                                  
==================================================================================================
Total params: 23,553
Trainable params: 23,553
Non-trainable params: 0
__________________________________________________________________________________________________
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_1 in the SavedModel.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Assets written to: hps_tf_triton_dense.model/assets
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Assets written to: hps_tf_triton_dense.model/assets
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="create-the-inference-graph-with-hps-lookuplayer">
<h2>Create the inference graph with HPS LookupLayer<a class="headerlink" href="#create-the-inference-graph-with-hps-lookuplayer" title="Permalink to this headline"></a></h2>
<p>In order to use HPS in the inference stage, we need to create a inference model graph which is almost the same as the train graph except that <code class="docutils literal notranslate"><span class="pre">tf.nn.embedding_lookup</span></code> is replaced by <code class="docutils literal notranslate"><span class="pre">hps.LookupLayer</span></code>. The trained dense model graph can be loaded directly, while the embedding weights should be converted to the formats required by HPS.</p>
<p>We can then save the inference model graph, which will be ready to be loaded for inference deployment. Please note that the inference SavedModel that leverages HPS will be deployed with the Triton TensorFlow backend, thus implicit initialization of HPS should be enabled by specifying <code class="docutils literal notranslate"><span class="pre">ps_config_file</span></code> and <code class="docutils literal notranslate"><span class="pre">global_batch_size</span></code> in the constructor of  <code class="docutils literal notranslate"><span class="pre">hps.LookupLayer</span></code>. For more information, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/api/initialize.html">HPS Initialize</a>.</p>
<p>To this end, we need to create a JSON configuration file and specify the details of the embedding tables for the models to be deployed. We only show how to deploy a model that has one embedding table here, and it can support multiple models with multiple embedding tables actually.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%writefile</span> hps_tf_triton.json
<span class="p">{</span>
    <span class="s2">&quot;supportlonglong&quot;</span><span class="p">:</span> <span class="n">true</span><span class="p">,</span>
    <span class="s2">&quot;models&quot;</span><span class="p">:</span> <span class="p">[{</span>
        <span class="s2">&quot;model&quot;</span><span class="p">:</span> <span class="s2">&quot;hps_tf_triton&quot;</span><span class="p">,</span>
        <span class="s2">&quot;sparse_files&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;/hugectr/hps_tf/notebooks/model_repo/hps_tf_triton/hps_tf_triton_sparse_0.model&quot;</span><span class="p">],</span>
        <span class="s2">&quot;num_of_worker_buffer_in_pool&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;embedding_table_names&quot;</span><span class="p">:[</span><span class="s2">&quot;sparse_embedding1&quot;</span><span class="p">],</span>
        <span class="s2">&quot;embedding_vecsize_per_table&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">16</span><span class="p">],</span>
        <span class="s2">&quot;maxnum_catfeature_query_per_table_per_sample&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">],</span>
        <span class="s2">&quot;default_value_for_each_table&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">],</span>
        <span class="s2">&quot;deployed_device_list&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="s2">&quot;max_batch_size&quot;</span><span class="p">:</span> <span class="mi">1024</span><span class="p">,</span>
        <span class="s2">&quot;cache_refresh_percentage_per_iteration&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="s2">&quot;hit_rate_threshold&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="s2">&quot;gpucacheper&quot;</span><span class="p">:</span> <span class="mf">1.0</span><span class="p">,</span>
        <span class="s2">&quot;gpucache&quot;</span><span class="p">:</span> <span class="n">true</span>
        <span class="p">}</span>
    <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing hps_tf_triton.json
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">triton_model_repo</span> <span class="o">=</span> <span class="s2">&quot;/hugectr/hps_tf/notebooks/model_repo/hps_tf_triton/&quot;</span>

<span class="k">class</span> <span class="nc">InferenceModel</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">slot_num</span><span class="p">,</span>
                 <span class="n">embed_vec_size</span><span class="p">,</span>
                 <span class="n">dense_dim</span><span class="p">,</span>
                 <span class="n">dense_model_path</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">InferenceModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span> <span class="o">=</span> <span class="n">slot_num</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_vec_size</span> <span class="o">=</span> <span class="n">embed_vec_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_dim</span> <span class="o">=</span> <span class="n">dense_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lookup_layer</span> <span class="o">=</span> <span class="n">hps</span><span class="o">.</span><span class="n">LookupLayer</span><span class="p">(</span><span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;hps_tf_triton&quot;</span><span class="p">,</span> 
                                            <span class="n">table_id</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
                                            <span class="n">emb_vec_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_vec_size</span><span class="p">,</span>
                                            <span class="n">emb_vec_dtype</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;tf_vector_type&quot;</span><span class="p">],</span>
                                            <span class="n">ps_config_file</span> <span class="o">=</span> <span class="n">triton_model_repo</span> <span class="o">+</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;ps_config_file&quot;</span><span class="p">],</span>
                                            <span class="n">global_batch_size</span> <span class="o">=</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;global_batch_size&quot;</span><span class="p">],</span>
                                            <span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;lookup&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">dense_model_path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lookup_layer</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">embedding_vector</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_vec_size</span><span class="p">])</span>
        <span class="n">logit</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_model</span><span class="p">([</span><span class="n">embedding_vector</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">logit</span>

    <span class="k">def</span> <span class="nf">summary</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">slot_num</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;tf_key_type&quot;</span><span class="p">]),</span>
                  <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)]</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">call</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_and_save_inference_graph</span><span class="p">(</span><span class="n">args</span><span class="p">):</span> 
    <span class="n">model</span> <span class="o">=</span> <span class="n">InferenceModel</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;slot_num&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;embed_vec_size&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;dense_dim&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;dense_model_path&quot;</span><span class="p">])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">model</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;slot_num&quot;</span><span class="p">],</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;tf_key_type&quot;</span><span class="p">]),</span>
               <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;dense_dim&quot;</span><span class="p">],</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)])</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;saved_path&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">convert_to_sparse_model</span><span class="p">(</span><span class="n">embeddings_weights</span><span class="p">,</span> <span class="n">embedding_table_path</span><span class="p">,</span> <span class="n">embedding_vec_size</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s2">&quot;mkdir -p </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">embedding_table_path</span><span class="p">))</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">/key&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">embedding_table_path</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">key_file</span><span class="p">,</span> \
        <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">/emb_vector&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">embedding_table_path</span><span class="p">),</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">vec_file</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">embeddings_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">vec</span> <span class="o">=</span> <span class="n">embeddings_weights</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        <span class="n">key_struct</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="s1">&#39;q&#39;</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
        <span class="n">vec_struct</span> <span class="o">=</span> <span class="n">struct</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">embedding_vec_size</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;f&quot;</span><span class="p">,</span> <span class="o">*</span><span class="n">vec</span><span class="p">)</span>
        <span class="n">key_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">key_struct</span><span class="p">)</span>
        <span class="n">vec_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">vec_struct</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">convert_to_sparse_model</span><span class="p">(</span><span class="n">embedding_weights</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;embedding_table_path&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;embed_vec_size&quot;</span><span class="p">])</span>
<span class="n">create_and_save_inference_graph</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;model_2&quot;
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_4 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 lookup (LookupLayer)           (None, 5, 16)        0           [&#39;input_4[0][0]&#39;]                
                                                                                                  
 tf.reshape_1 (TFOpLambda)      (None, 80)           0           [&#39;lookup[0][0]&#39;]                 
                                                                                                  
 input_5 (InputLayer)           [(None, 10)]         0           []                               
                                                                                                  
 model_1 (Functional)           (None, 1)            23553       [&#39;tf.reshape_1[0][0]&#39;,           
                                                                  &#39;input_5[0][0]&#39;]                
                                                                                                  
==================================================================================================
Total params: 23,553
Trainable params: 23,553
Non-trainable params: 0
__________________________________________________________________________________________________
INFO:tensorflow:Assets written to: hps_tf_triton_tf_saved_model/assets
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Assets written to: hps_tf_triton_tf_saved_model/assets
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="id1">
<h2>Deploy SavedModel using HPS with Triton TensorFlow Backend<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>In order to deploy the inference SavedModel with the Triton TensorFlow backend, we need to create the model repository and define the <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code> first. Please note that some required portions (i.e., the input and output tensors) of the model configuration are generated automatically by Triton (see <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#auto-generated-model-configuration">Auto-Generated Model Configuration</a>), so you do NOT need to specify them explicitly in <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>mkdir -p model_repo/hps_tf_triton/1
<span class="o">!</span>mv hps_tf_triton_tf_saved_model model_repo/hps_tf_triton/1/model.savedmodel
<span class="o">!</span>mv hps_tf_triton_sparse_0.model model_repo/hps_tf_triton
<span class="o">!</span>mv hps_tf_triton.json model_repo/hps_tf_triton
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%writefile</span> model_repo/hps_tf_triton/config.pbtxt
<span class="n">name</span><span class="p">:</span> <span class="s2">&quot;hps_tf_triton&quot;</span>
<span class="n">platform</span><span class="p">:</span> <span class="s2">&quot;tensorflow_savedmodel&quot;</span>
<span class="n">max_batch_size</span><span class="p">:</span><span class="mi">1024</span>
<span class="nb">input</span> <span class="p">[</span>
  <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;input_6&quot;</span>
    <span class="n">data_type</span><span class="p">:</span> <span class="n">TYPE_INT64</span>
    <span class="n">dims</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">]</span>
  <span class="p">},</span>
  <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;input_7&quot;</span>
    <span class="n">data_type</span><span class="p">:</span> <span class="n">TYPE_FP32</span>
    <span class="n">dims</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">]</span>
<span class="n">output</span> <span class="p">[</span>
  <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;output_1&quot;</span>
    <span class="n">data_type</span><span class="p">:</span> <span class="n">TYPE_FP32</span>
    <span class="n">dims</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">]</span>
<span class="n">version_policy</span><span class="p">:</span> <span class="p">{</span>
        <span class="n">specific</span><span class="p">:{</span><span class="n">versions</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="p">},</span>
<span class="n">instance_group</span> <span class="p">[</span>
  <span class="p">{</span>
    <span class="n">count</span><span class="p">:</span> <span class="mi">1</span>
    <span class="n">kind</span> <span class="p">:</span> <span class="n">KIND_GPU</span>
    <span class="n">gpus</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Writing model_repo/hps_tf_triton/config.pbtxt
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>tree model_repo/hps_tf_triton
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Blue">model_repo/hps_tf_triton</span>
├── <span class=" -Color -Color-Bold -Color-Bold-Blue">1</span>
│   └── <span class=" -Color -Color-Bold -Color-Bold-Blue">model.savedmodel</span>
│       ├── <span class=" -Color -Color-Bold -Color-Bold-Blue">assets</span>
│       ├── keras_metadata.pb
│       ├── saved_model.pb
│       └── <span class=" -Color -Color-Bold -Color-Bold-Blue">variables</span>
│           ├── variables.data-00000-of-00001
│           └── variables.index
├── config.pbtxt
├── hps_tf_triton.json
└── <span class=" -Color -Color-Bold -Color-Bold-Blue">hps_tf_triton_sparse_0.model</span>
    ├── emb_vector
    └── key

5 directories, 8 files
</pre></div>
</div>
</div>
</div>
<p>We can then launch the Triton inference server using the TensorFlow backend. Please note that <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> is utilized to load the custom TensorFlow operations (i.e., HPS related operations) into Triton. For more information, please refer to <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/user_guide/custom_operations.md#tensorflow">TensorFlow Custom Operations in Triton</a>.</p>
<p>Note: <code class="docutils literal notranslate"><span class="pre">Since</span> <span class="pre">Background</span> <span class="pre">processes</span> <span class="pre">not</span> <span class="pre">supported</span> <span class="pre">by</span> <span class="pre">Jupyter,</span> <span class="pre">please</span> <span class="pre">launch</span> <span class="pre">the</span> <span class="pre">Triton</span> <span class="pre">Server</span> <span class="pre">according</span> <span class="pre">to</span> <span class="pre">the</span> <span class="pre">following</span> <span class="pre">command</span> <span class="pre">independently</span> <span class="pre">in</span> <span class="pre">the</span> <span class="pre">background</span></code>.</p>
<blockquote>
<div><p><strong>LD_PRELOAD=/usr/local/lib/python3.8/dist-packages/merlin_hps-1.0.0-py3.8-linux-x86_64.egg/hierarchical_parameter_server/lib/libhierarchical_parameter_server.so tritonserver –model-repository=/hugectr/hps_tf/notebooks/model_repo –backend-config=tensorflow,version=2 –load-model=hps_tf_triton –model-control-mode=explicit</strong></p>
</div></blockquote>
<p>We can then send the requests to the Triton inference server using the HTTP client. Please note that HPS will be initialized implicitly when the first request is processed at the server side, and the latency can be higher than that of later requests.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>curl localhost:8000/v2/models/hps_tf_triton/config
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&quot;name&quot;:&quot;hps_tf_triton&quot;,&quot;platform&quot;:&quot;tensorflow_savedmodel&quot;,&quot;backend&quot;:&quot;tensorflow&quot;,&quot;version_policy&quot;:{&quot;specific&quot;:{&quot;versions&quot;:[1]}},&quot;max_batch_size&quot;:1024,&quot;input&quot;:[{&quot;name&quot;:&quot;input_6&quot;,&quot;data_type&quot;:&quot;TYPE_INT64&quot;,&quot;format&quot;:&quot;FORMAT_NONE&quot;,&quot;dims&quot;:[5],&quot;is_shape_tensor&quot;:false,&quot;allow_ragged_batch&quot;:false,&quot;optional&quot;:false},{&quot;name&quot;:&quot;input_7&quot;,&quot;data_type&quot;:&quot;TYPE_FP32&quot;,&quot;format&quot;:&quot;FORMAT_NONE&quot;,&quot;dims&quot;:[10],&quot;is_shape_tensor&quot;:false,&quot;allow_ragged_batch&quot;:false,&quot;optional&quot;:false}],&quot;output&quot;:[{&quot;name&quot;:&quot;output_1&quot;,&quot;data_type&quot;:&quot;TYPE_FP32&quot;,&quot;dims&quot;:[1],&quot;label_filename&quot;:&quot;&quot;,&quot;is_shape_tensor&quot;:false}],&quot;batch_input&quot;:[],&quot;batch_output&quot;:[],&quot;optimization&quot;:{&quot;priority&quot;:&quot;PRIORITY_DEFAULT&quot;,&quot;input_pinned_memory&quot;:{&quot;enable&quot;:true},&quot;output_pinned_memory&quot;:{&quot;enable&quot;:true},&quot;gather_kernel_buffer_threshold&quot;:0,&quot;eager_batching&quot;:false},&quot;dynamic_batching&quot;:{&quot;preferred_batch_size&quot;:[1024],&quot;max_queue_delay_microseconds&quot;:0,&quot;preserve_ordering&quot;:false,&quot;priority_levels&quot;:0,&quot;default_priority_level&quot;:0,&quot;priority_queue_policy&quot;:{}},&quot;instance_group&quot;:[{&quot;name&quot;:&quot;hps_tf_triton_0&quot;,&quot;kind&quot;:&quot;KIND_GPU&quot;,&quot;count&quot;:1,&quot;gpus&quot;:[0],&quot;secondary_devices&quot;:[],&quot;profile&quot;:[],&quot;passive&quot;:false,&quot;host_policy&quot;:&quot;&quot;}],&quot;default_model_filename&quot;:&quot;model.savedmodel&quot;,&quot;cc_model_filenames&quot;:{},&quot;metric_tags&quot;:{},&quot;parameters&quot;:{},&quot;model_warmup&quot;:[]}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">num_gpu</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_gpu</span><span class="p">)))</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tritonclient.http</span> <span class="k">as</span> <span class="nn">httpclient</span>
<span class="kn">from</span> <span class="nn">tritonclient.utils</span> <span class="kn">import</span> <span class="o">*</span>


<span class="k">def</span> <span class="nf">send_inference_requests</span><span class="p">(</span><span class="n">num_requests</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
    <span class="n">triton_client</span> <span class="o">=</span> <span class="n">httpclient</span><span class="o">.</span><span class="n">InferenceServerClient</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="s2">&quot;localhost:8000&quot;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">triton_client</span><span class="o">.</span><span class="n">is_server_live</span><span class="p">()</span>
    <span class="n">triton_client</span><span class="o">.</span><span class="n">get_model_repository_index</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_requests</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;--------------------------Request </span><span class="si">{}</span><span class="s2">--------------------------&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
        <span class="n">key_tensor</span><span class="p">,</span> <span class="n">dense_tensor</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">generate_random_samples</span><span class="p">(</span><span class="n">num_samples</span><span class="p">,</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;vocabulary_range_per_slot&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;dense_dim&quot;</span><span class="p">])</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">httpclient</span><span class="o">.</span><span class="n">InferInput</span><span class="p">(</span><span class="s2">&quot;input_6&quot;</span><span class="p">,</span> 
                                  <span class="n">key_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                  <span class="n">np_to_triton_dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">)),</span>
            <span class="n">httpclient</span><span class="o">.</span><span class="n">InferInput</span><span class="p">(</span><span class="s2">&quot;input_7&quot;</span><span class="p">,</span> 
                                  <span class="n">dense_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                  <span class="n">np_to_triton_dtype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)),</span>
        <span class="p">]</span>

        <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">key_tensor</span><span class="p">)</span>
        <span class="n">inputs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">dense_tensor</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">httpclient</span><span class="o">.</span><span class="n">InferRequestedOutput</span><span class="p">(</span><span class="s2">&quot;output_1&quot;</span><span class="p">)</span>
        <span class="p">]</span>

        <span class="c1"># print(&quot;Input key tensor is \n{}&quot;.format(key_tensor))</span>
        <span class="c1"># print(&quot;Input dense tensor is \n{}&quot;.format(dense_tensor))</span>
        <span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;hps_tf_triton&quot;</span>
        <span class="k">with</span> <span class="n">httpclient</span><span class="o">.</span><span class="n">InferenceServerClient</span><span class="p">(</span><span class="s2">&quot;localhost:8000&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">client</span><span class="p">:</span>
            <span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span>
                                    <span class="n">inputs</span><span class="p">,</span>
                                    <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">get_response</span><span class="p">()</span>

            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Response details:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">send_inference_requests</span><span class="p">(</span><span class="n">num_requests</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">128</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GET /v2/health/live, headers None
&lt;HTTPSocketPoolResponse status=200 headers={&#39;content-length&#39;: &#39;0&#39;, &#39;content-type&#39;: &#39;text/plain&#39;}&gt;
POST /v2/repository/index, headers None

&lt;HTTPSocketPoolResponse status=200 headers={&#39;content-type&#39;: &#39;application/json&#39;, &#39;content-length&#39;: &#39;56&#39;}&gt;
bytearray(b&#39;[{&quot;name&quot;:&quot;hps_tf_triton&quot;,&quot;version&quot;:&quot;1&quot;,&quot;state&quot;:&quot;READY&quot;}]&#39;)
--------------------------Request 0--------------------------
Response details:
{&#39;model_name&#39;: &#39;hps_tf_triton&#39;, &#39;model_version&#39;: &#39;1&#39;, &#39;outputs&#39;: [{&#39;name&#39;: &#39;output_1&#39;, &#39;datatype&#39;: &#39;FP32&#39;, &#39;shape&#39;: [128, 1], &#39;parameters&#39;: {&#39;binary_data_size&#39;: 512}}]}
--------------------------Request 1--------------------------
Response details:
{&#39;model_name&#39;: &#39;hps_tf_triton&#39;, &#39;model_version&#39;: &#39;1&#39;, &#39;outputs&#39;: [{&#39;name&#39;: &#39;output_1&#39;, &#39;datatype&#39;: &#39;FP32&#39;, &#39;shape&#39;: [128, 1], &#39;parameters&#39;: {&#39;binary_data_size&#39;: 512}}]}
--------------------------Request 2--------------------------
Response details:
{&#39;model_name&#39;: &#39;hps_tf_triton&#39;, &#39;model_version&#39;: &#39;1&#39;, &#39;outputs&#39;: [{&#39;name&#39;: &#39;output_1&#39;, &#39;datatype&#39;: &#39;FP32&#39;, &#39;shape&#39;: [128, 1], &#39;parameters&#39;: {&#39;binary_data_size&#39;: 512}}]}
--------------------------Request 3--------------------------
Response details:
{&#39;model_name&#39;: &#39;hps_tf_triton&#39;, &#39;model_version&#39;: &#39;1&#39;, &#39;outputs&#39;: [{&#39;name&#39;: &#39;output_1&#39;, &#39;datatype&#39;: &#39;FP32&#39;, &#39;shape&#39;: [128, 1], &#39;parameters&#39;: {&#39;binary_data_size&#39;: 512}}]}
--------------------------Request 4--------------------------
Response details:
{&#39;model_name&#39;: &#39;hps_tf_triton&#39;, &#39;model_version&#39;: &#39;1&#39;, &#39;outputs&#39;: [{&#39;name&#39;: &#39;output_1&#39;, &#39;datatype&#39;: &#39;FP32&#39;, &#39;shape&#39;: [128, 1], &#39;parameters&#39;: {&#39;binary_data_size&#39;: 512}}]}
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="deploy-tf-trt-savedmodel-using-hps-with-triton-tensorflow-backend">
<h2>Deploy TF-TRT SavedModel using HPS with Triton TensorFlow Backend<a class="headerlink" href="#deploy-tf-trt-savedmodel-using-hps-with-triton-tensorflow-backend" title="Permalink to this headline"></a></h2>
<p>We can leverage TF-TRT to optimize the above inference TF SavedModel. The <code class="docutils literal notranslate"><span class="pre">hps.LookupLayer</span></code> will fall back to the TF ops while the TensorRT engine will be built to execute the dense network. The optimized TF-TRT SavedModel can still be deployed with Triton TensorFlow backend.</p>
<p>The TF-TRT SavedModel is be placed in the folder <code class="docutils literal notranslate"><span class="pre">&quot;model_repo/hps_tf_triton/2/&quot;</span></code> and the <code class="docutils literal notranslate"><span class="pre">config.pbtxt</span></code> file is updated correspondingly to load the version 2 of the inference model, i.e., the TF-TRT optimized one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Build TF-TRT SavedModel</span>
<span class="kn">from</span> <span class="nn">tensorflow.python.compiler.tensorrt</span> <span class="kn">import</span> <span class="n">trt_convert</span> <span class="k">as</span> <span class="n">trt</span>

<span class="n">ORIGINAL_MODEL_PATH</span> <span class="o">=</span> <span class="s2">&quot;model_repo/hps_tf_triton/1/model.savedmodel&quot;</span>
<span class="n">NEW_MODEL_PATH</span> <span class="o">=</span> <span class="s2">&quot;model_repo/hps_tf_triton/2/model.savedmodel&quot;</span>

<span class="c1"># Instantiate the TF-TRT converter</span>
<span class="n">converter</span> <span class="o">=</span> <span class="n">trt</span><span class="o">.</span><span class="n">TrtGraphConverterV2</span><span class="p">(</span>
   <span class="n">input_saved_model_dir</span><span class="o">=</span><span class="n">ORIGINAL_MODEL_PATH</span><span class="p">,</span>
   <span class="n">precision_mode</span><span class="o">=</span><span class="n">trt</span><span class="o">.</span><span class="n">TrtPrecisionMode</span><span class="o">.</span><span class="n">FP32</span>
<span class="p">)</span>

<span class="c1"># Convert the model into TRT compatible segments</span>
<span class="n">trt_func</span> <span class="o">=</span> <span class="n">converter</span><span class="o">.</span><span class="n">convert</span><span class="p">()</span>
<span class="n">converter</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">generate_random_samples</span><span class="p">(</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;global_batch_size&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;vocabulary_range_per_slot&quot;</span><span class="p">],</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;dense_dim&quot;</span><span class="p">])</span>
<span class="n">keys</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
<span class="n">dense_features</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">(</span><span class="n">dense_features</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">input_fn</span><span class="p">():</span>
   <span class="k">yield</span> <span class="p">[</span><span class="n">keys</span><span class="p">,</span> <span class="n">dense_features</span><span class="p">]</span>

<span class="n">converter</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">input_fn</span><span class="o">=</span><span class="n">input_fn</span><span class="p">)</span>
<span class="n">converter</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">output_saved_model_dir</span><span class="o">=</span><span class="n">NEW_MODEL_PATH</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Linked TensorRT version: (8, 4, 2)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Linked TensorRT version: (8, 4, 2)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Loaded TensorRT version: (8, 4, 2)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Loaded TensorRT version: (8, 4, 2)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Clearing prior device assignments in loaded saved model
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-11-23 01:37:22.924379: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 1
2022-11-23 01:37:22.924537: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2022-11-23 01:37:22.928272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30991 MB memory:  -&gt; device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0
INFO:tensorflow:Clearing prior device assignments in loaded saved model
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Automatic mixed precision has been deactivated.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Automatic mixed precision has been deactivated.
2022-11-23 01:37:23.028482: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count &gt;= 8, compute capability &gt;= 0.0): 1
2022-11-23 01:37:23.028568: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session
2022-11-23 01:37:23.031909: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30991 MB memory:  -&gt; device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0
2022-11-23 01:37:23.048593: W tensorflow/compiler/tf2tensorrt/convert/trt_optimization_pass.cc:198] Calibration with FP32 or FP16 is not implemented. Falling back to use_calibration = False.Note that the default value of use_calibration is True.
2022-11-23 01:37:23.049761: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:952] 

################################################################################
TensorRT unsupported/non-converted OP Report:
	- NoOp -&gt; 2x
	- Placeholder -&gt; 2x
	- Identity -&gt; 1x
	- Init -&gt; 1x
	- Lookup -&gt; 1x
	- Reshape -&gt; 1x
--------------------------------------------------------------------------------
	- Total nonconverted OPs: 8
	- Total nonconverted OP Types: 6
For more information see https://docs.nvidia.com/deeplearning/frameworks/tf-trt-user-guide/index.html#supported-ops.
################################################################################

2022-11-23 01:37:23.049860: W tensorflow/compiler/tf2tensorrt/segment/segment.cc:1280] The environment variable TF_TRT_MAX_ALLOWED_ENGINES=20 has no effect since there are only 1 TRT Engines with  at least minimum_segment_size=3 nodes.
2022-11-23 01:37:23.049893: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:799] Number of TensorRT candidate segments: 1
2022-11-23 01:37:23.050667: I tensorflow/compiler/tf2tensorrt/convert/convert_graph.cc:916] Replaced segment 0 consisting of 9 nodes by TRTEngineOp_000_000.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>TRTEngineOP Name                 Device        # Nodes # Inputs      # Outputs     Input DTypes       Output Dtypes      Input Shapes       Output Shapes     
================================================================================================================================================================
TRTEngineOp_000_000              device:GPU:0  10      2             1             [&#39;float32&#39;, &#39;f ... [&#39;float32&#39;]        [[-1, 80], [-1 ... [[-1, 1]]         

	- BiasAdd: 2x
	- ConcatV2: 1x
	- Const: 5x
	- MatMul: 2x

================================================================================================================================================================
[*] Total number of TensorRT engines: 1
[*] % of OPs Converted: 50.00% [10/20]

=====================================================HPS Parse====================================================
[HCTR][01:37:23.329][INFO][RK0][main]: dense_file is not specified using default: 
[HCTR][01:37:23.329][INFO][RK0][main]: num_of_refresher_buffer_in_pool is not specified using default: 1
[HCTR][01:37:23.329][INFO][RK0][main]: maxnum_des_feature_per_sample is not specified using default: 26
[HCTR][01:37:23.329][INFO][RK0][main]: refresh_delay is not specified using default: 0
[HCTR][01:37:23.329][INFO][RK0][main]: refresh_interval is not specified using default: 0
====================================================HPS Create====================================================
[HCTR][01:37:23.329][INFO][RK0][main]: Creating HashMap CPU database backend...
[HCTR][01:37:23.329][DEBUG][RK0][main]: Created blank database backend in local memory!
[HCTR][01:37:23.329][INFO][RK0][main]: Volatile DB: initial cache rate = 1
[HCTR][01:37:23.329][INFO][RK0][main]: Volatile DB: cache missed embeddings = 0
[HCTR][01:37:23.329][DEBUG][RK0][main]: Created raw model loader in local memory!
[HCTR][01:37:23.745][INFO][RK0][main]: Table: hps_et.hps_tf_triton.sparse_embedding1; cached 50000 / 50000 embeddings in volatile database (HashMapBackend); load: 50000 / 18446744073709551615 (0.00%).
[HCTR][01:37:23.745][DEBUG][RK0][main]: Real-time subscribers created!
[HCTR][01:37:23.745][INFO][RK0][main]: Creating embedding cache in device 0.
[HCTR][01:37:23.753][INFO][RK0][main]: Model name: hps_tf_triton
[HCTR][01:37:23.753][INFO][RK0][main]: Number of embedding tables: 1
[HCTR][01:37:23.753][INFO][RK0][main]: Use GPU embedding cache: True, cache size percentage: 1.000000
[HCTR][01:37:23.753][INFO][RK0][main]: Use I64 input key: True
[HCTR][01:37:23.753][INFO][RK0][main]: Configured cache hit rate threshold: 1.000000
[HCTR][01:37:23.753][INFO][RK0][main]: The size of thread pool: 80
[HCTR][01:37:23.753][INFO][RK0][main]: The size of worker memory pool: 3
[HCTR][01:37:23.753][INFO][RK0][main]: The size of refresh memory pool: 1
[HCTR][01:37:23.753][INFO][RK0][main]: The refresh percentage : 0.200000
[HCTR][01:37:23.778][DEBUG][RK0][main]: Created raw model loader in local memory!
[HCTR][01:37:23.814][INFO][RK0][main]: EC initialization for model: &quot;hps_tf_triton&quot;, num_tables: 1
[HCTR][01:37:23.814][INFO][RK0][main]: EC initialization on device: 0
[HCTR][01:37:23.815][INFO][RK0][main]: Creating lookup session for hps_tf_triton on device: 0
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2022-11-23 01:37:23.818078: I tensorflow/compiler/tf2tensorrt/common/utils.cc:104] Linked TensorRT version: 8.4.2
2022-11-23 01:37:23.818150: I tensorflow/compiler/tf2tensorrt/common/utils.cc:106] Loaded TensorRT version: 8.4.2
2022-11-23 01:37:28.749149: I tensorflow/compiler/tf2tensorrt/convert/convert_nodes.cc:1275] [TF-TRT] Sparse compute capability is enabled.
2022-11-23 01:37:28.814132: E tensorflow/compiler/tf2tensorrt/utils/trt_logger.cc:86] DefaultLogger 1: [wrapper.cpp::CublasWrapper::85] Error Code 1: Cublas (Could not initialize cublas. Please check CUDA installation.)
2022-11-23 01:37:28.817575: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:1061] TF-TRT Warning: Engine creation for TRTEngineOp_000_000 failed. The native segment will be used instead. Reason: INTERNAL: Failed to build TensorRT engine
2022-11-23 01:37:28.817694: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:894] TF-TRT Warning: Engine retrieval for input shapes: [[1024,80], [1024,10]] failed. Running native segment for TRTEngineOp_000_000
2022-11-23 01:37:28.823806: W tensorflow/compiler/tf2tensorrt/kernels/trt_engine_op.cc:894] TF-TRT Warning: Engine retrieval for input shapes: [[1024,80], [1024,10]] failed. Running native segment for TRTEngineOp_000_000
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Assets written to: model_repo/hps_tf_triton/2/model.savedmodel/assets
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:tensorflow:Assets written to: model_repo/hps_tf_triton/2/model.savedmodel/assets
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%writefile</span> model_repo/hps_tf_triton/config.pbtxt
<span class="n">name</span><span class="p">:</span> <span class="s2">&quot;hps_tf_triton&quot;</span>
<span class="n">platform</span><span class="p">:</span> <span class="s2">&quot;tensorflow_savedmodel&quot;</span>
<span class="n">max_batch_size</span><span class="p">:</span><span class="mi">1024</span>
<span class="nb">input</span> <span class="p">[</span>
  <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;input_6&quot;</span>
    <span class="n">data_type</span><span class="p">:</span> <span class="n">TYPE_INT64</span>
    <span class="n">dims</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">]</span>
  <span class="p">},</span>
  <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;input_7&quot;</span>
    <span class="n">data_type</span><span class="p">:</span> <span class="n">TYPE_FP32</span>
    <span class="n">dims</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">]</span>
<span class="n">output</span> <span class="p">[</span>
  <span class="p">{</span>
    <span class="n">name</span><span class="p">:</span> <span class="s2">&quot;output_1&quot;</span>
    <span class="n">data_type</span><span class="p">:</span> <span class="n">TYPE_FP32</span>
    <span class="n">dims</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">]</span>
<span class="n">version_policy</span><span class="p">:</span> <span class="p">{</span>
        <span class="n">specific</span><span class="p">:{</span><span class="n">versions</span><span class="p">:</span> <span class="mi">2</span><span class="p">}</span>
<span class="p">},</span>
<span class="n">instance_group</span> <span class="p">[</span>
  <span class="p">{</span>
    <span class="n">count</span><span class="p">:</span> <span class="mi">1</span>
    <span class="n">kind</span> <span class="p">:</span> <span class="n">KIND_GPU</span>
    <span class="n">gpus</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
  <span class="p">}</span>
<span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Overwriting model_repo/hps_tf_triton/config.pbtxt
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>tree model_repo/hps_tf_triton
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span><span class=" -Color -Color-Bold -Color-Bold-Blue">model_repo/hps_tf_triton</span>
├── <span class=" -Color -Color-Bold -Color-Bold-Blue">1</span>
│   └── <span class=" -Color -Color-Bold -Color-Bold-Blue">model.savedmodel</span>
│       ├── <span class=" -Color -Color-Bold -Color-Bold-Blue">assets</span>
│       ├── keras_metadata.pb
│       ├── saved_model.pb
│       └── <span class=" -Color -Color-Bold -Color-Bold-Blue">variables</span>
│           ├── variables.data-00000-of-00001
│           └── variables.index
├── <span class=" -Color -Color-Bold -Color-Bold-Blue">2</span>
│   └── <span class=" -Color -Color-Bold -Color-Bold-Blue">model.savedmodel</span>
│       ├── <span class=" -Color -Color-Bold -Color-Bold-Blue">assets</span>
│       │   └── trt-serialized-engine.TRTEngineOp_000_000
│       ├── saved_model.pb
│       └── <span class=" -Color -Color-Bold -Color-Bold-Blue">variables</span>
│           ├── variables.data-00000-of-00001
│           └── variables.index
├── config.pbtxt
├── hps_tf_triton.json
└── <span class=" -Color -Color-Bold -Color-Bold-Blue">hps_tf_triton_sparse_0.model</span>
    ├── emb_vector
    └── key

9 directories, 12 files
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Release the occupied GPU memory by TensorFlow and Keras</span>
<span class="kn">from</span> <span class="nn">numba</span> <span class="kn">import</span> <span class="n">cuda</span>
<span class="n">cuda</span><span class="o">.</span><span class="n">select_device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">cuda</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>We can then launch the Triton inference server using the TensorFlow backend using the same command in the background. Please remember to kill the previous <code class="docutils literal notranslate"><span class="pre">tritonserver</span></code> process completely before launching it again. Otherwise, there could be out of memory errors.</p>
<p>When the triton server is succesfully launched, we can then send the requests to it using the HTTP client again.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>curl localhost:8000/v2/models/hps_tf_triton/config
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&quot;name&quot;:&quot;hps_tf_triton&quot;,&quot;platform&quot;:&quot;tensorflow_savedmodel&quot;,&quot;backend&quot;:&quot;tensorflow&quot;,&quot;version_policy&quot;:{&quot;specific&quot;:{&quot;versions&quot;:[2]}},&quot;max_batch_size&quot;:1024,&quot;input&quot;:[{&quot;name&quot;:&quot;input_6&quot;,&quot;data_type&quot;:&quot;TYPE_INT64&quot;,&quot;format&quot;:&quot;FORMAT_NONE&quot;,&quot;dims&quot;:[5],&quot;is_shape_tensor&quot;:false,&quot;allow_ragged_batch&quot;:false,&quot;optional&quot;:false},{&quot;name&quot;:&quot;input_7&quot;,&quot;data_type&quot;:&quot;TYPE_FP32&quot;,&quot;format&quot;:&quot;FORMAT_NONE&quot;,&quot;dims&quot;:[10],&quot;is_shape_tensor&quot;:false,&quot;allow_ragged_batch&quot;:false,&quot;optional&quot;:false}],&quot;output&quot;:[{&quot;name&quot;:&quot;output_1&quot;,&quot;data_type&quot;:&quot;TYPE_FP32&quot;,&quot;dims&quot;:[1],&quot;label_filename&quot;:&quot;&quot;,&quot;is_shape_tensor&quot;:false}],&quot;batch_input&quot;:[],&quot;batch_output&quot;:[],&quot;optimization&quot;:{&quot;priority&quot;:&quot;PRIORITY_DEFAULT&quot;,&quot;input_pinned_memory&quot;:{&quot;enable&quot;:true},&quot;output_pinned_memory&quot;:{&quot;enable&quot;:true},&quot;gather_kernel_buffer_threshold&quot;:0,&quot;eager_batching&quot;:false},&quot;dynamic_batching&quot;:{&quot;preferred_batch_size&quot;:[1024],&quot;max_queue_delay_microseconds&quot;:0,&quot;preserve_ordering&quot;:false,&quot;priority_levels&quot;:0,&quot;default_priority_level&quot;:0,&quot;priority_queue_policy&quot;:{}},&quot;instance_group&quot;:[{&quot;name&quot;:&quot;hps_tf_triton_0&quot;,&quot;kind&quot;:&quot;KIND_GPU&quot;,&quot;count&quot;:1,&quot;gpus&quot;:[0],&quot;secondary_devices&quot;:[],&quot;profile&quot;:[],&quot;passive&quot;:false,&quot;host_policy&quot;:&quot;&quot;}],&quot;default_model_filename&quot;:&quot;model.savedmodel&quot;,&quot;cc_model_filenames&quot;:{},&quot;metric_tags&quot;:{},&quot;parameters&quot;:{},&quot;model_warmup&quot;:[]}
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">send_inference_requests</span><span class="p">(</span><span class="n">num_requests</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">num_samples</span> <span class="o">=</span> <span class="mi">128</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>GET /v2/health/live, headers None
&lt;HTTPSocketPoolResponse status=200 headers={&#39;content-length&#39;: &#39;0&#39;, &#39;content-type&#39;: &#39;text/plain&#39;}&gt;
POST /v2/repository/index, headers None

&lt;HTTPSocketPoolResponse status=200 headers={&#39;content-type&#39;: &#39;application/json&#39;, &#39;content-length&#39;: &#39;56&#39;}&gt;
bytearray(b&#39;[{&quot;name&quot;:&quot;hps_tf_triton&quot;,&quot;version&quot;:&quot;2&quot;,&quot;state&quot;:&quot;READY&quot;}]&#39;)
--------------------------Request 0--------------------------
Response details:
{&#39;model_name&#39;: &#39;hps_tf_triton&#39;, &#39;model_version&#39;: &#39;2&#39;, &#39;outputs&#39;: [{&#39;name&#39;: &#39;output_1&#39;, &#39;datatype&#39;: &#39;FP32&#39;, &#39;shape&#39;: [128, 1], &#39;parameters&#39;: {&#39;binary_data_size&#39;: 512}}]}
--------------------------Request 1--------------------------
Response details:
{&#39;model_name&#39;: &#39;hps_tf_triton&#39;, &#39;model_version&#39;: &#39;2&#39;, &#39;outputs&#39;: [{&#39;name&#39;: &#39;output_1&#39;, &#39;datatype&#39;: &#39;FP32&#39;, &#39;shape&#39;: [128, 1], &#39;parameters&#39;: {&#39;binary_data_size&#39;: 512}}]}
--------------------------Request 2--------------------------
Response details:
{&#39;model_name&#39;: &#39;hps_tf_triton&#39;, &#39;model_version&#39;: &#39;2&#39;, &#39;outputs&#39;: [{&#39;name&#39;: &#39;output_1&#39;, &#39;datatype&#39;: &#39;FP32&#39;, &#39;shape&#39;: [128, 1], &#39;parameters&#39;: {&#39;binary_data_size&#39;: 512}}]}
--------------------------Request 3--------------------------
Response details:
{&#39;model_name&#39;: &#39;hps_tf_triton&#39;, &#39;model_version&#39;: &#39;2&#39;, &#39;outputs&#39;: [{&#39;name&#39;: &#39;output_1&#39;, &#39;datatype&#39;: &#39;FP32&#39;, &#39;shape&#39;: [128, 1], &#39;parameters&#39;: {&#39;binary_data_size&#39;: 512}}]}
--------------------------Request 4--------------------------
Response details:
{&#39;model_name&#39;: &#39;hps_tf_triton&#39;, &#39;model_version&#39;: &#39;2&#39;, &#39;outputs&#39;: [{&#39;name&#39;: &#39;output_1&#39;, &#39;datatype&#39;: &#39;FP32&#39;, &#39;shape&#39;: [128, 1], &#39;parameters&#39;: {&#39;binary_data_size&#39;: 512}}]}
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="sok_to_hps_dlrm_demo.html" class="btn btn-neutral float-left" title="SOK to HPS DLRM Demo" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../../hierarchical_parameter_server/api/index.html" class="btn btn-neutral float-right" title="Hierarchical Parameter Server API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: main
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="../../../v3.8/index.html">v3.8</a></dd>
      <dd><a href="../../../v3.9/index.html">v3.9</a></dd>
      <dd><a href="../../../v3.9.1/index.html">v3.9.1</a></dd>
      <dd><a href="../../../v4.0/index.html">v4.0</a></dd>
      <dd><a href="../../../v4.1/index.html">v4.1</a></dd>
      <dd><a href="../../../v4.1.1/index.html">v4.1.1</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="hps_tensorflow_triton_deployment_demo.html">main</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
    <!-- Theme Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NVJ1Y1YJHK"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-NVJ1Y1YJHK', {
          'anonymize_ip': false,
      });
    </script> 

</body>
</html>