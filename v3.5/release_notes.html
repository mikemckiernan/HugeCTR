<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Release Notes &mdash; Merlin HugeCTR  documentation</title><link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/togglebutton.css" type="text/css" />
      <link rel="stylesheet" href="_static/mystnb.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script >let toggleHintShow = 'Click to show';</script>
        <script >let toggleHintHide = 'Click to hide';</script>
        <script >let toggleOpenOnPrint = 'true';</script>
        <script src="_static/togglebutton.js"></script>
        <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Contributing to HugeCTR" href="hugectr_contributor_guide.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> Merlin HugeCTR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="hugectr_user_guide.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="hugectr_core_features.html">Core Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="hugectr_feature_details_intro.html">Features in Detail</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/index.html">Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/multi-modal-data/index.html">Multi-modal Example Notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="api/index.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="additional_resources.html">Additional Resources</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Release Notes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#what-s-new-in-version-3-5">What’s New in Version 3.5</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-s-new-in-version-3-4-1">What’s New in Version 3.4.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-s-new-in-version-3-4">What’s New in Version 3.4</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-s-new-in-version-3-3-1">What’s New in Version 3.3.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-s-new-in-version-3-3">What’s New in Version 3.3</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-s-new-in-version-3-2-1">What’s New in Version 3.2.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-s-new-in-version-3-2">What’s New in Version 3.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-s-new-in-version-3-1">What’s New in Version 3.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-s-new-in-version-3-0-1">What’s New in Version 3.0.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="#whats-new-in-version-3-0">What’s New in Version 3.0</a></li>
<li class="toctree-l2"><a class="reference internal" href="#what-s-new-in-version-2-3">What’s New in Version 2.3</a></li>
<li class="toctree-l2"><a class="reference internal" href="#known-issues">Known Issues</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Merlin HugeCTR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Release Notes</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="tex2jax_ignore mathjax_ignore section" id="release-notes">
<h1>Release Notes<a class="headerlink" href="#release-notes" title="Permalink to this headline"></a></h1>
<div class="section" id="what-s-new-in-version-3-5">
<h2>What’s New in Version 3.5<a class="headerlink" href="#what-s-new-in-version-3-5" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>HPS interface encapsulation and exporting as library</strong>: We encapsulate the Hierarchical Parameter Server(HPS) interfaces and deliver it as a standalone library. Besides, we prodvide HPS Python APIs and demonstrate the usage with a notebook. For more information, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_parameter_server.html">Hierarchical Parameter Server</a> and <a class="reference internal" href="notebooks/hps_demo.html"><span class="doc std std-doc">HPS Demo</span></a>.</p></li>
<li><p><strong>Hierarchical Parameter Server Triton Backend</strong>: The HPS Backend is a framework for embedding vectors looking up on large-scale embedding tables that was designed to effectively use GPU memory to accelerate the looking up by decoupling the embedding tables and embedding cache from the end-to-end inference pipeline of the deep recommendation model. For more information, please refer to the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/v3.5/samples">samples</a> directory of the HugeCTR backend for Triton Inference Server repository.</p></li>
<li><p><strong>SOK pip release</strong>: SOK pip releases on <a class="reference external" href="https://pypi.org/project/merlin-sok/">https://pypi.org/project/merlin-sok/</a>. Now users can install SOK via <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">merlin-sok</span></code>.</p></li>
<li><p><strong>Joint loss and multi-tasks training support:</strong>: We support joint loss in training so that users can train with multiple labels and tasks with different weights. See the MMoE sample in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.5/samples/mmoe">samples/mmoe</a> directory of the repository to learn the usage.</p></li>
<li><p><strong>HugeCTR documentation on web page</strong>: Now users can visit our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/">web documentation</a>.</p></li>
<li><p><strong>ONNX converter enhancement:</strong>: We enable converting <code class="docutils literal notranslate"><span class="pre">MultiCrossEntropyLoss</span></code> and <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code> layers to ONNX to support multi-label inference. For more information, please refer to the HugeCTR to ONNX Converter information in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.5/onnx_converter">onnx_converter</a> directory of the repository.</p></li>
<li><p><strong>HDFS python API enhancement</strong>:</p>
<ul>
<li><p>Simplified <code class="docutils literal notranslate"><span class="pre">DataSourceParams</span></code> so that users do not need to provide all the paths before they are really necessary. Now users only have to pass <code class="docutils literal notranslate"><span class="pre">DataSourceParams</span></code> once when creating a solver.</p></li>
<li><p>Later paths will be automatically regarded as local paths or HDFS paths depending on the <code class="docutils literal notranslate"><span class="pre">DataSourceParams</span></code> setting. See <a class="reference internal" href="notebooks/training_with_hdfs.html"><span class="doc std std-doc">notebook</span></a> for usage.</p></li>
</ul>
</li>
<li><p><strong>HPS performance optimization</strong>: We use better method to  determine partition number in database backends in HPS.</p></li>
<li><p><strong>Bug fixing</strong>: HugeCTR input layer now can take dense_dim greater than 1000.</p></li>
</ul>
</div>
<div class="section" id="what-s-new-in-version-3-4-1">
<h2>What’s New in Version 3.4.1<a class="headerlink" href="#what-s-new-in-version-3-4-1" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>Support mixed precision inference for dataset with multiple labels</strong>: We enable FP16 for the <code class="docutils literal notranslate"><span class="pre">Softmax</span></code> layer and support mixed precision for multi-label inference. For more information, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#inference-api">Inference API</a>.</p></li>
<li><p><strong>Support multi-GPU offline inference with Python API</strong>: We support multi-GPU offline inference with the Python interface, which can leverage <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_parameter_server.html">Hierarchical Parameter Server</a> and enable concurrent execution on multiple devices. For more information, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#inference-api">Inference API</a> and <a class="reference internal" href="notebooks/multi_gpu_offline_inference.html"><span class="doc std std-doc">Multi-GPU Offline Inference Notebook</span></a>.</p></li>
<li><p><strong>Introduction to metadata.json</strong>: We add the introduction to <code class="docutils literal notranslate"><span class="pre">_metadata.json</span></code> for Parquet datasets. For more information, please refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/index.html">Parquet</a>.</p></li>
<li><p><strong>Documents and tool for workspace size per GPU estimation</strong>: we add a tool that is named the <code class="docutils literal notranslate"><span class="pre">embedding_workspace_calculator</span></code> to help calculate the value for <code class="docutils literal notranslate"><span class="pre">workspace_size_per_gpu_in_mb</span></code> that is required by hugectr.SparseEmbedding. For more information, please refer to the <a class="reference external" href="http://README.md">README.md</a> file in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.4.1/tools/embedding_workspace_calculator">tools/embedding_workspace_calculator</a> directory of the repository and <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/QAList.html">QA 24</a> in the documentation.</p></li>
<li><p><strong>Improved Debugging Capability</strong>: The old logging system, which was flagged as deprecated for some time has been removed. All remaining log messages and outputs have been revised and migrated to the new logging system (base/debug/logging.hpp/cpp). During this revision, we also adjusted log levels for log messages throughout the entire codebase to improve visibility of relevant information.</p></li>
<li><p><strong>Support HDFS Parameter Server in Training</strong>:</p>
<ul>
<li><p>Decoupled HDFS in Merlin containers to make the HDFS support more flexible. Users can now compile HDFS related functionalities optionally.</p></li>
<li><p>Now supports loading and dumping models and optimizer states from HDFS.</p></li>
<li><p>Added a <a class="reference internal" href="notebooks/training_with_hdfs.html"><span class="doc std std-doc">notebook</span></a> to show how to use HugeCTR with HDFS.</p></li>
</ul>
</li>
<li><p><strong>Support Multi-hot Inference on Hugectr Backend</strong>: We support categorical input in multi-hot format for HugeCTR Backend inference.</p></li>
<li><p><strong>Multi-label inference with mixed precision</strong>: Mixed precision training is enabled for softmax layer.</p></li>
<li><p><strong>Python Script and documentation demonstrating how to analyze model files</strong>: In this release, we provide a script to retrieve vocabulary information from model file. Please find more details on the README in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.4.1/tools/model_analyzer">tools/model_analyzer</a> directory of the repository.</p></li>
<li><p><strong>Bug Fixing</strong>:</p>
<ul>
<li><p>Mirror strategy bug in SOK (see in <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/291">https://github.com/NVIDIA-Merlin/HugeCTR/issues/291</a>)</p></li>
<li><p>Can’t import sparse operation kit in <a class="reference external" href="http://nvcr.io/nvidia/merlin/merlin-tensorflow-training:22.04">nvcr.io/nvidia/merlin/merlin-tensorflow-training:22.04</a> (see in <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/296">https://github.com/NVIDIA-Merlin/HugeCTR/issues/296</a>)</p></li>
<li><p>HPS: Fixed access violation that can occur during initialization when not configuring a volatile DB.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="what-s-new-in-version-3-4">
<h2>What’s New in Version 3.4<a class="headerlink" href="#what-s-new-in-version-3-4" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>Support for Building HugeCTR with the Unified Merlin Container</strong>: HugeCTR can now be built using our unified Merlin container. For more information, refer to our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_contributor_guide.html">Contributor Guide</a>.</p></li>
<li><p><strong>Hierarchical Parameter Server (HPS) Enhancements</strong>:</p>
<ul>
<li><p><strong>New Missing Key (Embedding Table Entries) Insertion Feature</strong>: Using a simple flag, it is now possible to configure HugeCTR with missing keys (embedding table entries). During lookup, these missing keys will automatically be inserted into volatile database layers such as the Redis and Hashmap backends.</p></li>
<li><p><strong>Asynchronous Timestamp Refresh</strong>: To allow time-based eviction to take place, it is now possible to enable timestamp refreshing for frequently used embeddings. Once enabled, refreshing is handled asynchronously using background threads, which won’t block your inference jobs. For most applications, the associated performance impact from enabling this feature is barely noticeable.</p></li>
<li><p><strong>HDFS (Hadoop Distributed File System) Parameter Server Support During Training</strong>:</p>
<ul>
<li><p>We’re introducing a new DataSourceParams API, which is a python API that can be used to specify the file system and paths to data and model files.</p></li>
<li><p>We’ve added support for loading data from HDFS to the local file system for HugeCTR training.</p></li>
<li><p>We’ve added support for dumping trained model and optimizer states into HDFS.</p></li>
</ul>
</li>
<li><p><strong>New Load API Capabilities</strong>: In addition to being able to deploy new models, the HugeCTR Backend’s <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md#load">Load API</a> can now be used to update the dense parameters for models and corresponding embedding inference cache online.</p></li>
</ul>
</li>
<li><p><strong>Sparse Operation Kit (SOK) Enhancements</strong>:</p>
<ul>
<li><p><strong>Mixed Precision Training</strong>: Enabling mixed precision training using TensorFlow’s pattern to enhance the training performance and lessen memory usage is now possible.</p></li>
<li><p><strong>DLRM Benchmark</strong>: DLRM is a standard benchmark for recommendation model training, so we added a new notebook. Refer to the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.4/sparse_operation_kit/documents/tutorials/DLRM_Benchmark">sparse_operation_kit/documents/tutorials/DLRM_Benchmark</a> directory of the repository. The notebook shows how to address the performance of SOK on this benchmark.</p></li>
<li><p><strong>Uint32_t / int64_t key dtype Support in SOK</strong>: Int64 or uint32 can now be used as the key data type for SOK’s embedding. Int64 is the default.</p></li>
<li><p><strong>TensorFlow Initializers Support</strong>: We now support the TensorFlow native initializer within SOK, such as <code class="docutils literal notranslate"><span class="pre">sok.All2AllDenseEmbedding(embedding_initializer=tf.keras.initializers.RandomUniform())</span></code>. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/api/embeddings/dense/all2all.html">All2All Dense Embedding</a>.</p></li>
</ul>
</li>
<li><p><strong>Documentation Enhancements</strong></p>
<ul>
<li><p>We’ve revised several of our notebooks and readme files to improve readability and accessibility.</p></li>
<li><p>We’ve revised the SOK docker setup instructions to indicate that HugeCTR setup issues can be resolved using the <code class="docutils literal notranslate"><span class="pre">--shm-size</span></code> setting within docker.</p></li>
<li><p>Although HugeCTR is designed for scalability, having a robust machine is not necessary for smaller workloads and testing. We’ve documented the required specifications for notebook testing environments. For more information, refer to our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/notebooks/index.html#system-specifications">README for HugeCTR Jupyter Demo Notebooks</a>.</p></li>
</ul>
</li>
<li><p><strong>Inference Enhancements</strong>：We now support HugeCTR inference for managing multiple tasks. When the label dimension is the number of binary classification tasks and <code class="docutils literal notranslate"><span class="pre">MultiCrossEntropyLoss</span></code> is employed during training, the shape of inference results will be <code class="docutils literal notranslate"><span class="pre">(batch_size*num_batches,</span> <span class="pre">label_dim)</span></code>. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#inference-api">Inference API</a>.</p></li>
<li><p><strong>Embedding Cache Issue Resolution</strong>: The embedding cache issue for very small embedding tables has been resolved.</p></li>
</ul>
</div>
<div class="section" id="what-s-new-in-version-3-3-1">
<h2>What’s New in Version 3.3.1<a class="headerlink" href="#what-s-new-in-version-3-3-1" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>Hierarchical Parameter Server (HPS) Enhancements</strong>:</p>
<ul>
<li><p><strong>HugeCTR Backend Enhancements</strong>: The HugeCTR Backend is now fully compatible with the <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md">Triton model control protocol</a>, so new model configurations can be simply added to the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend#independent-parameter-server-configuration">HPS configuration file</a>. The HugeCTR Backend will continue to support online deployments of new models using the Triton Load API. However, with this enhancement, old models can be recycled online using the <a class="reference external" href="https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_repository.md#unload">Triton Unload API</a>.</p></li>
<li><p><strong>Simplified Database Backend</strong>: Multi-nodes, single-node, and all other kinds of volatile database backends can now be configured using the same configuration object.</p></li>
<li><p><strong>Multi-Threaded Optimization of Redis Code</strong>: The speedup of HugeCTR version 3.3.1 is 2.3 times faster than HugeCTR version 3.3.</p></li>
<li><p><strong>Additional HPS Enhancements and Fixes</strong>:</p>
<ul>
<li><p>You can now build the HPS test environment and implement unit tests for each component.</p></li>
<li><p>You’ll no longer encounter the access violation issue when updating Apache Kafka online.</p></li>
<li><p>The parquet data reader no longer incorrectly parses the index of categorical features when multiple embedded tables are being used.</p></li>
<li><p>The HPS Redis Backend overflow is now invoked during single insertions.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>New GroupDenseLayer</strong>: We’re introducing a new GroupDenseLayer. It can be used to group fused fully connected layers when constructing the model graph. A simplified Python interface is provided for adjusting the number of layers and specifying the output dimensions in each layer, which makes it easy to leverage the highly-optimized fused fully connected layers in HugeCTR. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#groupdenselayer">GroupDenseLayer</a>.</p></li>
<li><p><strong>Global Fixes</strong>:</p>
<ul>
<li><p>A warning message now appears when attempting to launch a multi-process job before importing the mpi.</p></li>
<li><p>When running with embedding training cache, a massive log is no longer generated.</p></li>
<li><p>Legacy conda information has been removed from the HugeCTR documentation.</p></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="what-s-new-in-version-3-3">
<h2>What’s New in Version 3.3<a class="headerlink" href="#what-s-new-in-version-3-3" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>Hierarchical Parameter Server (HPS) Enhancements</strong>:</p>
<ul>
<li><p><strong>Support for Incremental Model Updates</strong>: HPS now supports incremental model updates via Apache Kafka (a distributed event streaming platform) message queues. With this enhancement, HugeCTR can now be connected with Apache Kafka deployments to update models in real time during training and inference. For more information, refer to the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/main/samples/hierarchical_deployment/hps_e2e_demo">Demo Notebok</a>.</p></li>
<li><p><strong>Improvements to the Memory Management</strong>: The Redis cluster and CPU memory database backends are now the primary sources for memory management. When performing incremental model updates, these memory database backends will automatically evict infrequently used embeddings as training progresses. The performance of the Redis cluster and CPU memory database backends have also been improved.</p></li>
<li><p><strong>New Asynchronous Refresh Mechanism</strong>: Support for asynchronous refreshing of incremental embedding keys into the embedding cache has been added. The Refresh operation will be triggered when completing the model version iteration or outputting incremental parameters from online training. The Distributed Database and Persistent Database will be updated by Apache Kafka. The GPU embedding cache will then refresh the values of the existing embedding keys and replace them with the latest incremental embedding vectors. For more information, refer to the <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend#hugectr-hierarchical-parameter-server">HPS README</a>.</p></li>
<li><p><strong>Configurable Backend Implementations for Databases</strong>: Backend implementations for databases are now fully configurable.</p></li>
<li><p><strong>Improvements to the JSON Interface Parser</strong>: The JSON interface parser can now handle inaccurate parameterization.</p></li>
<li><p><strong>More Meaningful Jabber</strong>: As requested, we’ve revised the log levels throughout the entire API database backend of the HPS. Selected configuration options are now printed entirely and uniformly to the log. Errors provide more verbose information about pending issues.</p></li>
</ul>
</li>
<li><p><strong>Sparse Operation Kit (SOK) Enhancements</strong>:</p>
<ul>
<li><p><strong>TensorFlow (TF) 1.15 Support</strong>: SOK can now be used with TensorFlow 1.15. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/get_started/get_started.html#tensorflow-1-15">README</a>.</p></li>
<li><p><strong>Dedicated CUDA Stream</strong>: A dedicated CUDA stream is now used for SOK’s Ops, so this may help to eliminate kernel interleaving.</p></li>
<li><p><strong>New pip Installation Option</strong>: SOK can now be installed using the <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">SparseOperationKit</span></code> command. See more in our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/intro_link.html#installation">instructions</a>). With this install option, root access to compile SOK is no longer required and python scripts don’t need to be copied.</p></li>
<li><p><strong>Visible Device Configuration Support</strong>：<code class="docutils literal notranslate"><span class="pre">tf.config.set_visible_device</span></code> can now be used to set visible GPUs for each process. <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> can also be used. When <code class="docutils literal notranslate"><span class="pre">tf.distribute.Strategy</span></code> is used, the <code class="docutils literal notranslate"><span class="pre">tf.config.set_visible_device</span></code> argument shouldn’t be set.</p></li>
<li><p><strong>Hanging Issue Fix</strong>: There was a hanging issue in <code class="docutils literal notranslate"><span class="pre">tf.distribute.MirroredStrategy</span></code> when using TensorFlow version 2.4 and higher. However, this is no longer an issue when using TensorFlow version 2.5 and higher.</p></li>
</ul>
</li>
<li><p><strong>MLPerf v1.1 Integration Enhancements</strong>：</p>
<ul>
<li><p><strong>Precomputed Hybrid Embedding Indices</strong>：The necessary indices for hybrid embedding are now precomputed ahead of time and overlapped with previous iterations.</p></li>
<li><p><strong>Cached Eval Indices:</strong>：The hybrid embedding indices for eval are cached when applicable. Index re-computing is no longer needed at every eval iteration.</p></li>
<li><p><strong>MLP Weight/Data Gradients Calculation Overlap:</strong>：The weight gradients of MLP are calculated asynchronously with respect to the data gradients, enabling overlap between these two computations.</p></li>
<li><p><strong>Improved Compute/Communication Overlap:</strong>：Enhancements to the overlap between the compute and communication has been implemented to improve training throughput.</p></li>
<li><p><strong>Fused Weight Conversion:</strong>：The FP32-to-FP16 conversion of the weights are now fused into the SGD optimizer, saving trips to memory.</p></li>
<li><p><strong>GraphScheduler Support:</strong>：GrapScheduler was added to control the cudaGraph launch timing. With GraphScheduler, the gap between adjacent cudaGraphs has been eliminated.</p></li>
</ul>
</li>
<li><p><strong>Multi-Node Training Support Enhancements</strong>：You can now perform multi-node training on the cluster with non-RDMA hardware by setting the <code class="docutils literal notranslate"><span class="pre">AllReduceAlgo.NCCL</span></code> value for the <code class="docutils literal notranslate"><span class="pre">all_reduce_algo</span></code> argument. For more information, refer to the details for the <code class="docutils literal notranslate"><span class="pre">all_reduce_algo</span></code> argument in the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#solver">CreateSolver API</a>.</p></li>
<li><p><strong>Support for Model Naming During Model Dumping</strong>: You can now specify names for models with the <code class="docutils literal notranslate"><span class="pre">CreateSolver</span></code>training API, which will be dumped to the JSON configuration file with the <code class="docutils literal notranslate"><span class="pre">Model.graph_to_json</span></code> API. This will facilitate the Triton deployment of saved HugeCTR models, as well as help to distinguish between models when Apache Kafka sends parameters from training to inference.</p></li>
<li><p><strong>Fine-Grained Control Accessibility Enhancements for Embedding Layers</strong>: We’ve added fine-grained control accessibility to embedding layers. Using the <code class="docutils literal notranslate"><span class="pre">Model.freeze_embedding</span></code> and <code class="docutils literal notranslate"><span class="pre">Model.unfreeze_embedding</span></code> APIs, embedding layer weights can be frozen and unfrozen. Additionally, weights for multiple embedding layers can be loaded independently, making it possible to load pre-trained embeddings for a particular layer. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#model">Model API</a> and <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/v3.3/notebooks/hugectr_criteo.ipynb">Section 3.4 of the HugeCTR Criteo Notebook</a>.</p></li>
</ul>
</div>
<div class="section" id="what-s-new-in-version-3-2-1">
<h2>What’s New in Version 3.2.1<a class="headerlink" href="#what-s-new-in-version-3-2-1" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>GPU Embedding Cache Optimization</strong>: The performance of the GPU embedding cache for the standalone module has been optimized. With this enhancement, the performance of small to medium batch sizes has improved significantly. We’re not introducing any changes to the interface for the GPU embedding cache, so don’t worry about making changes to any existing code that uses this standalone module. For more information, refer to the <code class="docutils literal notranslate"><span class="pre">ReadMe.md</span></code> file in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.2.1/gpu_cache">gpu_cache</a> directory of the repository.</p></li>
<li><p><strong>Model Oversubscription Enhancements</strong>: We’re introducing a new host memory cache (HMEM-Cache) component for the model oversubscription feature. When configured properly, incremental training can be efficiently performed on models with large embedding tables that exceed the host memory. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_core_features.html#embedding-training-cache">Host Memory Cache in MOS</a>. Additionally, we’ve enhanced the Python interface for model oversubscription by replacing the <code class="docutils literal notranslate"><span class="pre">use_host_memory_ps</span></code> parameter with a <code class="docutils literal notranslate"><span class="pre">ps_types</span></code> parameter and adding a <code class="docutils literal notranslate"><span class="pre">sparse_models</span></code> parameter. For more information about these changes, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#solver">HugeCTR Python Interface</a>.</p></li>
<li><p><strong>Debugging Enhancements</strong>: We’re introducing new debugging features such as multi-level logging, as well as kernel debugging functions. We’re also making our error messages more informative so that users know exactly how to resolve issues related to their training and inference code. For more information, refer to the comments in the header files, which are available at HugeCTR/include/base/debug.</p></li>
<li><p><strong>Enhancements to the Embedding Key Insertion Mechanism for the Embedding Cache</strong>: Missing embedding keys can now be asynchronously inserted into the embedding cache. To enable automatically, set the hit rate threshold within the configuration file. When the actual hit rate of the embedding cache is higher than the hit rate threshold that the user set or vice versa, the embedding cache will insert the missing embedding key asynchronously.</p></li>
<li><p><strong>Parameter Server Enhancements</strong>: We’re introducing a new “in memory” database that utilizes the local CPU memory for storing and recalling embeddings and uses multi-threading to accelerate lookup and storage. You can now also use the combined CPU-accessible memory of your Redis cluster to store embeddings. We improved the performance for the “persistent” storage and retrieving embeddings from RocksDB using structured column families, as well as added support for creating hierarchical storage such as Redis as distributed cache. You don’t have to worry about updating your Parameter Server configurations to take advantage of these enhancements.</p></li>
<li><p><strong>Slice Layer Internalization Enhancements</strong>: The Slice layer for the branch toplogy can now be abstracted away in the Python interface. A model graph analysis will be conducted to resolve the tensor dependency and the Slice layer will be internally inserted if the same tensor is consumed more than once to form the branch topology. For more information about how to construct a model graph using branches without the Slice layer, refer to the Getting Started section of the repository README and the <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#layers">Slice Layer</a> information.</p></li>
</ul>
</div>
<div class="section" id="what-s-new-in-version-3-2">
<h2>What’s New in Version 3.2<a class="headerlink" href="#what-s-new-in-version-3-2" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>New HugeCTR to ONNX Converter</strong>: We’re introducing a new HugeCTR to ONNX converter in the form of a Python package. All graph configuration files are required and model weights must be formatted as inputs. You can specify where you want to save the converted ONNX model. You can also convert sparse embedding models. For more information, refer to the HugeCTR to ONNX Converter information in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.2/onnx_converter">onnx_converter</a> directory and the <a class="reference internal" href="notebooks/hugectr2onnx_demo.html"><span class="doc std std-doc">HugeCTR2ONNX Demo Notebook</span></a>.</p></li>
<li><p><strong>New Hierarchical Storage Mechanism on the Parameter Server (POC)</strong>: We’ve implemented a hierarchical storage mechanism between local SSDs and CPU memory. As a result, embedding tables no longer have to be stored in the local CPU memory. The distributed Redis cluster is being implemented as a CPU cache to store larger embedding tables and interact with the GPU embedding cache directly. The local RocksDB serves as a query engine to back up the complete embedding table on the local SSDs and assist the Redis cluster with looking up missing embedding keys. For more information about how this works, refer to our <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/blob/main/docs/architecture.md#distributed-deployment-with-hierarchical-hugectr-parameter-server">HugeCTR Backend documentation</a></p></li>
<li><p><strong>Parquet Format Support within the Data Generator</strong>: The HugeCTR data generator now supports the parquet format, which can be configured easily using the Python API. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#data-generator-api">Data Generator API</a>.</p></li>
<li><p><strong>Python Interface Support for the Data Generator</strong>: The data generator has been enabled within the HugeCTR Python interface. The parameters associated with the data generator have been encapsulated into the <code class="docutils literal notranslate"><span class="pre">DataGeneratorParams</span></code> struct, which is required to initialize the <code class="docutils literal notranslate"><span class="pre">DataGenerator</span></code> instance. You can use the data generator’s Python APIs to easily generate the Norm, Parquet, or Raw dataset formats with the desired distribution of sparse keys. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#data-generator-api">Data Generator API</a> and the data generator samples in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.2/tools/data_generator">tools/data_generator</a> directory of the repository.</p></li>
<li><p><strong>Improvements to the Formula of the Power Law Simulator within the Data Generator</strong>: We’ve modified the formula of the power law simulator within the data generator so that a positive alpha value is always produced, which will be needed for most use cases. The alpha values for <code class="docutils literal notranslate"><span class="pre">Long</span></code>, <code class="docutils literal notranslate"><span class="pre">Medium</span></code>, and <code class="docutils literal notranslate"><span class="pre">Short</span></code> within the power law distribution are 0.9, 1.1, and 1.3 respectively. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#data-generator-api">Data Generator API</a>.</p></li>
<li><p><strong>Support for Arbitrary Input and Output Tensors in the Concat and Slice Layers</strong>: The Concat and Slice layers now support any number of input and output tensors. Previously, these layers were limited to a maximum of four tensors.</p></li>
<li><p><strong>New Continuous Training Notebook</strong>: We’ve added a new notebook to demonstrate how to perform continuous training using the embedding training cache (also referred to as Embedding Training Cache) feature. For more information, refer to <a class="reference internal" href="notebooks/continuous_training.html"><span class="doc std std-doc">HugeCTR Continuous Training</span></a>.</p></li>
<li><p><strong>New HugeCTR Contributor Guide</strong>: We’ve added a new <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_contributor_guide.html">HugeCTR Contributor Guide</a> that explains how to contribute to HugeCTR, which may involve reporting and fixing a bug, introducing a new feature, or implementing a new or pending feature.</p></li>
<li><p><strong>Sparse Operation Kit (SOK) Enhancements</strong>: SOK now supports TensorFlow 2.5 and 2.6. We also added support for identity hashing, dynamic input, and Horovod within SOK. Lastly, we added a new <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/sparse_operation_kit/master/index.html">SOK docs set</a> to help you get started with SOK.</p></li>
</ul>
</div>
<div class="section" id="what-s-new-in-version-3-1">
<h2>What’s New in Version 3.1<a class="headerlink" href="#what-s-new-in-version-3-1" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>MLPerf v1.0 Integration</strong>: We’ve integrated MLPerf optimizations for DLRM training and enabled them as configurable options in Python interface. Specifically, we have incorporated AsyncRaw data reader, HybridEmbedding, FusedReluBiasFullyConnectedLayer, overlapped pipeline, holistic CUDA Graph and so on. The performance of 14-node DGX-A100 DLRM training with Python APIs is comparable to CLI usage. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html">HugeCTR Python Interface</a> and the sample DLRM program in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.1/samples/dlrm">samples/dlrm</a> directory of the repository.</p></li>
<li><p><strong>Python Interface Enhancements</strong>: We’ve enhanced the Python interface for HugeCTR so that you no longer have to manually create a JSON configuration file. Our Python APIs can now be used to create the computation graph. They can also be used to dump the model graph as a JSON object and save the model weights as binary files so that continuous training and inference can take place. We’ve added an Inference API that takes Norm or Parquet datasets as input to facilitate the inference process. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html">HugeCTR Python Interface</a> and <a class="reference internal" href="notebooks/hugectr_criteo.html"><span class="doc std std-doc">HugeCTR Criteo Notebook</span></a>.</p></li>
<li><p><strong>New Interface for Unified Embedding</strong>: We’re introducing a new interface to simplify the use of embeddings and datareaders. To help you specify the number of keys in each slot, we added <code class="docutils literal notranslate"><span class="pre">nnz_per_slot</span></code> and <code class="docutils literal notranslate"><span class="pre">is_fixed_length</span></code>. You can now directly configure how much memory usage you need by specifying <code class="docutils literal notranslate"><span class="pre">workspace_size_per_gpu_in_mb</span></code> instead of <code class="docutils literal notranslate"><span class="pre">max_vocabulary_size_per_gpu</span></code>. For convenience, <code class="docutils literal notranslate"><span class="pre">mean/sum</span></code> is used in combinators instead of 0 and 1. In cases where you don’t know which embedding type you should use, you can specify <code class="docutils literal notranslate"><span class="pre">use_hash_table</span></code> and let HugeCTR automatically select the embedding type based on your configuration. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html">HugeCTR Python Interface</a>.</p></li>
<li><p><strong>Multi-Node Support for Embedding Training Cache (ETC)</strong>: We’ve enabled multi-node support for the embedding training cache. You can now train a model with a terabyte-size embedding table using one node or multiple nodes even if the entire embedding table can’t fit into the GPU memory. We’re also introducing the host memory (HMEM) based parameter server (PS) along with its SSD-based counterpart. If the sparse model can fit into the host memory of each training node, the optimized HMEM-based PS can provide better model loading and dumping performance with a more effective bandwidth. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html">HugeCTR Python Interface</a>.</p></li>
<li><p><strong>Enhancements to the Multi-Nodes TensorFlow Plugin</strong>: The Multi-Nodes TensorFlow Plugin now supports multi-node synchronized training via tf.distribute.MultiWorkerMirroredStrategy. With minimal code changes, you can now easily scale your single GPU training to multi-node multi GPU training. The Multi-Nodes TensorFlow Plugin also supports multi-node synchronized training via Horovod. The inputs for embedding plugins are now data parallel, so the datareader no longer needs to preprocess data for different GPUs based on concrete embedding algorithms. For more information, see the <code class="docutils literal notranslate"><span class="pre">sparse_operation_kit_demo.ipynb</span></code> notebook in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.2/sparse_operation_kit/notebooks">sparse_operation_kit/notebooks</a> directory of the repository.</p></li>
<li><p><strong>NCF Model Support</strong>: We’ve added support for the NCF model, as well as the GMF and NeuMF variant models. With this enhancement, we’re introducing a new element-wise multiplication layer and HitRate evaluation metric. Sample code was added that demonstrates how to preprocess user-item interaction data and train a NCF model with it. New examples have also been added that demonstrate how to train NCF models using MovieLens datasets.</p></li>
<li><p><strong>DIN and DIEN Model Support</strong>: All of our layers support the DIN model. The following layers support the DIEN model: FusedReshapeConcat, FusedReshapeConcatGeneral, Gather, GRU, PReLUDice, ReduceMean, Scale, Softmax, and Sub. We also added sample code to demonstrate how to use the Amazon dataset to train the DIN model. See our sample programs in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.1/samples/din">samples/din</a> directory of the repository.</p></li>
<li><p><strong>Multi-Hot Support for Parquet Datasets</strong>: We’ve added multi-hot support for parquet datasets, so you can now train models with a paraquet dataset that contains both one hot and multi-hot slots.</p></li>
<li><p><strong>Mixed Precision (FP16) Support in More Layers</strong>: The MultiCross layer now supports mixed precision (FP16). All layers now support FP16.</p></li>
<li><p><strong>Mixed Precision (FP16) Support in Inference</strong>: We’ve added FP16 support for the inference pipeline. Therefore, dense layers can now adopt FP16 during inference.</p></li>
<li><p><strong>Optimizer State Enhancements for Continuous Training</strong>: You can now store optimizer states that are updated during continuous training as files, such as the Adam optimizer’s first moment (m) and second moment (v). By default, the optimizer states are initialized with zeros, but you can specify a set of optimizer state files to recover their previous values. For more information about <code class="docutils literal notranslate"><span class="pre">dense_opt_states_file</span></code> and <code class="docutils literal notranslate"><span class="pre">sparse_opt_states_file</span></code>, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html">Python Interface</a>.</p></li>
<li><p><strong>New Library File for GPU Embedding Cache Data</strong>: We’ve moved the header/source code of the GPU embedding cache data structure into a stand-alone folder. It has been compiled into a stand-alone library file. Similar to HugeCTR, your application programs can now be directly linked from this new library file for future use. For more information, refer to the <code class="docutils literal notranslate"><span class="pre">ReadMe.md</span></code> file in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v3.1/gpu_cache">gpu_cache</a> directory of the repository.</p></li>
<li><p><strong>Embedding Plugin Enhancements</strong>: We’ve moved all the embedding plugin files into a stand-alone folder. The embedding plugin can be used as a stand-alone python module, and works with TensorFlow to accelerate the embedding training process.</p></li>
<li><p><strong>Adagrad Support</strong>: Adagrad can now be used to optimize your embedding and network. To use it, change the optimizer type in the Optimizer layer and set the corresponding parameters.</p></li>
</ul>
</div>
<div class="section" id="what-s-new-in-version-3-0-1">
<h2>What’s New in Version 3.0.1<a class="headerlink" href="#what-s-new-in-version-3-0-1" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>New DLRM Inference Benchmark</strong>: We’ve added two detailed Jupyter notebooks to demonstrate how to train, deploy, and benchmark the performance of a deep learning recommendation model (DLRM) with HugeCTR. For more information, refer to our <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend/tree/v3.0.1/samples/dlrm">HugeCTR Inference Notebooks</a>.</p></li>
<li><p><strong>FP16 Optimization</strong>: We’ve optimized the DotProduct, ELU, and Sigmoid layers based on <code class="docutils literal notranslate"><span class="pre">__half2</span></code> vectorized loads and stores, improving their device memory bandwidth utilization. MultiCross, FmOrder2, ReduceSum, and Multiply are the only layers that still need to be optimized for FP16.</p></li>
<li><p><strong>Synthetic Data Generator Enhancements</strong>: We’ve enhanced our synthetic data generator so that it can generate uniformly distributed datasets, as well as power-law based datasets. You can now specify the <code class="docutils literal notranslate"><span class="pre">vocabulary_size</span></code> and <code class="docutils literal notranslate"><span class="pre">max_nnz</span></code> per categorical feature instead of across all categorial features. For more information, refer to our <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_user_guide.html#generating-synthetic-data-and-benchmarks">user guide</a>.</p></li>
<li><p><strong>Reduced Memory Allocation for Trained Model Exportation</strong>: To prevent the “Out of Memory” error message from displaying when exporting a trained model, which may include a very large embedding table, the amount of memory allocated by the related functions has been significantly reduced.</p></li>
<li><p><strong>Dropout Layer Enhancement</strong>: The Dropout layer is now compatible with CUDA Graph. The Dropout layer is using cuDNN by default so that it can be used with CUDA Graph.</p></li>
</ul>
</div>
<div class="section" id="whats-new-in-version-3-0">
<h2>What’s New in Version 3.0<a class="headerlink" href="#whats-new-in-version-3-0" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>Inference Support</strong>: To streamline the recommender system workflow, we’ve implemented a custom HugeCTR backend on the <a class="reference external" href="https://developer.nvidia.com/nvidia-triton-inference-server">NVIDIA Triton Inference Server</a>. The HugeCTR backend leverages the embedding cache and parameter server to efficiently manage embeddings of different sizes and models in a hierarchical manner. For more information, refer to <a class="reference external" href="https://github.com/triton-inference-server/hugectr_backend">our inference repository</a>.</p></li>
<li><p><strong>New High-Level API</strong>: You can now also construct and train your models using the Python interface with our new high-level API. For more information, refer to our preview example code in the <code class="docutils literal notranslate"><span class="pre">samples/preview</span></code> directory to grasp how this new API works.</p></li>
<li><p><strong><a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/hugectr_core_features.html#mixed-precision-training">FP16 Support</a> in More Layers</strong>: All the layers except <code class="docutils literal notranslate"><span class="pre">MultiCross</span></code> support mixed precision mode. We’ve also optimized some of the FP16 layer implementations based on vectorized loads and stores.</p></li>
<li><p><strong>Enhanced TensorFlow Embedding Plugin</strong>: Our embedding plugin now supports <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingHash</span></code> mode. With this enhancement, the DNN model no longer needs to be split into two parts since it now connects with the embedding op through <code class="docutils literal notranslate"><span class="pre">MirroredStrategy</span></code> within the embedding layer. For more information, see the <code class="docutils literal notranslate"><span class="pre">notebooks/embedding_plugin.ipynb</span></code> notebook.</p></li>
<li><p><strong>Extended Embedding Training Cache</strong>: We’ve extended the embedding training cache feature to support <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingHash</span></code> and <code class="docutils literal notranslate"><span class="pre">LocalizedSlotSparseEmbeddingHashOneHot</span></code>.</p></li>
<li><p><strong>Epoch-Based Training Enhancements</strong>: The <code class="docutils literal notranslate"><span class="pre">num_epochs</span></code> option in the <strong>Solver</strong> clause can now be used with the <code class="docutils literal notranslate"><span class="pre">Raw</span></code> dataset format.</p></li>
<li><p><strong>Deprecation of the <code class="docutils literal notranslate"><span class="pre">eval_batches</span></code> Parameter</strong>: The <code class="docutils literal notranslate"><span class="pre">eval_batches</span></code> parameter has been deprecated and replaced with the <code class="docutils literal notranslate"><span class="pre">max_eval_batches</span></code> and <code class="docutils literal notranslate"><span class="pre">max_eval_samples</span></code> parameters. In epoch mode, these parameters control the maximum number of evaluations. An error message will appear when attempting to use the <code class="docutils literal notranslate"><span class="pre">eval_batches</span></code> parameter.</p></li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">MultiplyLayer</span></code> Renamed</strong>: To clarify what the <code class="docutils literal notranslate"><span class="pre">MultiplyLayer</span></code> does, it was renamed to <code class="docutils literal notranslate"><span class="pre">WeightMultiplyLayer</span></code>.</p></li>
<li><p><strong>Optimized Initialization Time</strong>: HugeCTR’s initialization time, which includes the GEMM algorithm search and parameter initialization, was significantly reduced.</p></li>
<li><p><strong>Sample Enhancements</strong>: Our samples now rely upon the <a class="reference external" href="https://ailab.criteo.com/download-criteo-1tb-click-logs-dataset/">Criteo 1TB Click Logs dataset</a> instead of the Kaggle Display Advertising Challenge dataset. Our preprocessing scripts (Perl, Pandas, and NVTabular) have also been unified and simplified.</p></li>
<li><p><strong>Configurable DataReader Worker</strong>: You can now specify the number of data reader workers, which run in parallel, with the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> parameter. Its default value is 12. However, if you are using the Parquet data reader, you can’t configure the <code class="docutils literal notranslate"><span class="pre">num_workers</span></code> parameter since it always corresponds to the number of active GPUs.</p></li>
</ul>
</div>
<div class="section" id="what-s-new-in-version-2-3">
<h2>What’s New in Version 2.3<a class="headerlink" href="#what-s-new-in-version-2-3" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p><strong>New Python Interface</strong>: To enhance the interoperability with <a class="reference external" href="https://github.com/NVIDIA/NVTabular">NVTabular</a> and other Python-based libraries, we’re introducing a new Python interface for HugeCTR.</p></li>
<li><p><strong>HugeCTR Embedding with Tensorflow</strong>: To help users easily integrate HugeCTR’s optimized embedding into their Tensorflow workflow, we now offer the HugeCTR embedding layer as a Tensorflow plugin. To better understand how to install, use, and verify it, see our Jupyter notebook tutorial in file <code class="docutils literal notranslate"><span class="pre">notebooks/embedding_plugin.ipynb</span></code>. The notebook also demonstrates how you can create a new Keras layer, <code class="docutils literal notranslate"><span class="pre">EmbeddingLayer</span></code>, based on the <code class="docutils literal notranslate"><span class="pre">hugectr.py</span></code> file in the <code class="docutils literal notranslate"><span class="pre">tools/embedding_plugin/python</span></code> directory with the helper code that we provide.</p></li>
<li><p><strong>Embedding Training Cache</strong>: To enable a model with large embedding tables that exceeds the single GPU’s memory limit, we’ve added a new embedding training cache feature, giving you the ability to load a subset of an embedding table into the GPU in a coarse grained, on-demand manner during the training stage.</p></li>
<li><p><strong>TF32 Support</strong>: We’ve added TensorFloat-32 (TF32), a new math mode and third-generation of Tensor Cores, support on Ampere. TF32 uses the same 10-bit mantissa as FP16 to ensure accuracy while providing the same range as FP32 by using an 8-bit exponent. Since TF32 is an internal data type that accelerates FP32 GEMM computations with tensor cores, you can simply turn it on with a newly added configuration option. For more information, refer to <a class="reference external" href="https://nvidia-merlin.github.io/HugeCTR/master/api/python_interface.html#solver">Solver</a>.</p></li>
<li><p><strong>Enhanced AUC Implementation</strong>: To enhance the performance of our AUC computation on multi-node environments, we’ve redesigned our AUC implementation to improve how the computational load gets distributed across nodes.</p></li>
<li><p><strong>Epoch-Based Training</strong>: In addition to the <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> parameter, you can now set the <code class="docutils literal notranslate"><span class="pre">num_epochs</span></code> parameter in the <strong>Solver</strong> clause within the configuration file. This mode can only currently be used with <code class="docutils literal notranslate"><span class="pre">Norm</span></code> dataset formats and their corresponding file lists. All dataset formats will be supported in the future.</p></li>
<li><p><strong>New Multi-Node Training Tutorial</strong>: To better support multi-node training use cases, we’ve added a new step-by-step tutorial to the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/master/tutorial/multinode-training">tutorial/multinode-training</a> directory of our GitHub repository.</p></li>
<li><p><strong>Power Law Distribution Support with Data Generator</strong>: Because of the increased need for generating a random dataset whose categorical features follows the power-law distribution, we’ve revised our data generation tool to support this use case. For additional information, refer to the <code class="docutils literal notranslate"><span class="pre">--long-tail</span></code> description in the Generating Synthetic Data and Benchmarks section of the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/blob/v2.3/docs/hugectr_user_guide.md#generating-synthetic-data-and-benchmarks">docs/hugectr_user_guide.md</a> file in the repository.</p></li>
<li><p><strong>Multi-GPU Preprocessing Script for Criteo Samples</strong>: Multiple GPUs can now be used when preparing the dataset for the programs in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v2.3/samples">samples</a> directory of our GitHub repository. For more information, see how the <code class="docutils literal notranslate"><span class="pre">preprocess_nvt.py</span></code> program in the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/tree/v2.3/tools/criteo_script">tools/criteo_script</a> directory of the repository is used to preprocess the Criteo dataset for DCN, DeepFM, and W&amp;D samples.</p></li>
</ul>
</div>
<div class="section" id="known-issues">
<h2>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this headline"></a></h2>
<ul class="simple">
<li><p>HugeCTR uses NCCL to share data between ranks, and NCCL may require shared system memory for IPC and pinned (page-locked) system memory resources. When using NCCL inside a container, it is recommended that you increase these resources by issuing: <code class="docutils literal notranslate"><span class="pre">-shm-size=1g</span> <span class="pre">-ulimit</span> <span class="pre">memlock=-1</span></code>
See also <a class="reference external" href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#sharing-data">NCCL’s known issue</a>. And the <a class="reference external" href="https://github.com/NVIDIA-Merlin/HugeCTR/issues/243">GitHub issue</a>.</p></li>
<li><p>KafkaProducers startup will succeed, even if the target Kafka broker is unresponsive. In order to avoid data-loss in conjunction with streaming model updates from Kafka, you have to make sure that a sufficient number of Kafka brokers is up, working properly and reachable from the node where you run HugeCTR.</p></li>
<li><p>The number of data files in the file list should be no less than the number of data reader workers. Otherwise, different workers will be mapped to the same file and data loading does not progress as expected.</p></li>
<li><p>Joint Loss training hasn’t been supported with regularizer.</p></li>
</ul>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="hugectr_contributor_guide.html" class="btn btn-neutral float-left" title="Contributing to HugeCTR" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, NVIDIA.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    <span class="fa fa-book"> Other Versions</span>
    v: v3.5
    <span class="fa fa-caret-down"></span>
  </span>
  <div class="rst-other-versions">
    <dl>
      <dt>Tags</dt>
      <dd><a href="release_notes.html">v3.5</a></dd>
    </dl>
    <dl>
      <dt>Branches</dt>
      <dd><a href="../master/index.html">master</a></dd>
    </dl>
  </div>
</div><script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>