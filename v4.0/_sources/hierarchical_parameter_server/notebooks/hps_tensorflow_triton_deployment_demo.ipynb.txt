{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7adf4cf",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Deploy SavedModel using HPS with Triton TensorFlow Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac24a16",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates how to deploy the SavedModel that leverages HPS with [Triton TensorFlow backend](https://github.com/triton-inference-server/tensorflow_backend). It is recommended to run [hierarchical_parameter_server_demo.ipynb](hierarchical_parameter_server_demo.ipynb) before diving into this notebook.\n",
    "\n",
    "For more details about HPS APIs, please refer to [HPS APIs](https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/api/index.html). For more details about HPS, please refer to [HugeCTR Hierarchical Parameter Server (HPS)](https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435079b1",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Get HPS from NGC\n",
    "\n",
    "The HPS Python module is preinstalled in the 22.09 and later [Merlin TensorFlow Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-tensorflow): `nvcr.io/nvidia/merlin/merlin-tensorflow:22.09`.\n",
    "\n",
    "You can check the existence of the required libraries by running the following Python code after launching this container.\n",
    "\n",
    "```bash\n",
    "$ python3 -c \"import hierarchical_parameter_server as hps\"\n",
    "```\n",
    "\n",
    "The Triton TensorFlow backend is also available in this container."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a74924",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "First of all we specify the required configurations, e.g., the arguments needed for generating the dataset, the paths to save the model and the model parameters. We will use a deep neural network (DNN) model which has one embedding table and several dense layers in this notebook. Please note that there are two inputs here, one is the key tensor (one-hot) while the other is the dense feature tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3531ed85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] hierarchical_parameter_server is imported\n"
     ]
    }
   ],
   "source": [
    "import hierarchical_parameter_server as hps\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import struct\n",
    "\n",
    "args = dict()\n",
    "\n",
    "args[\"gpu_num\"] = 1                               # the number of available GPUs\n",
    "args[\"iter_num\"] = 10                             # the number of training iteration\n",
    "args[\"slot_num\"] = 5                              # the number of feature fields in this embedding layer\n",
    "args[\"embed_vec_size\"] = 16                       # the dimension of embedding vectors\n",
    "args[\"global_batch_size\"] = 1024                  # the globally batchsize for all GPUs\n",
    "args[\"max_vocabulary_size\"] = 50000\n",
    "args[\"vocabulary_range_per_slot\"] = [[0,10000],[10000,20000],[20000,30000],[30000,40000],[40000,50000]]\n",
    "args[\"dense_dim\"] = 10\n",
    "\n",
    "args[\"dense_model_path\"] = \"hps_tf_triton_dense.model\"\n",
    "args[\"ps_config_file\"] = \"hps_tf_triton.json\"\n",
    "args[\"embedding_table_path\"] = \"hps_tf_triton_sparse_0.model\"\n",
    "args[\"saved_path\"] = \"hps_tf_triton_tf_saved_model\"\n",
    "args[\"np_key_type\"] = np.int64\n",
    "args[\"np_vector_type\"] = np.float32\n",
    "args[\"tf_key_type\"] = tf.int64\n",
    "args[\"tf_vector_type\"] = tf.float32\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(args[\"gpu_num\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58bf8139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_samples(num_samples, vocabulary_range_per_slot, dense_dim, key_dtype = args[\"np_key_type\"]):\n",
    "    keys = list()\n",
    "    for vocab_range in vocabulary_range_per_slot:\n",
    "        keys_per_slot = np.random.randint(low=vocab_range[0], high=vocab_range[1], size=(num_samples, 1), dtype=key_dtype)\n",
    "        keys.append(keys_per_slot)\n",
    "    keys = np.concatenate(np.array(keys), axis = 1)\n",
    "    dense_features = np.random.random((num_samples, dense_dim)).astype(np.float32)\n",
    "    labels = np.random.randint(low=0, high=2, size=(num_samples, 1))\n",
    "    return keys, dense_features, labels\n",
    "\n",
    "def tf_dataset(keys, dense_features, labels, batchsize):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((keys, dense_features, labels))\n",
    "    dataset = dataset.batch(batchsize, drop_remainder=True)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d974399",
   "metadata": {},
   "source": [
    "## Train with native TF layers\n",
    "\n",
    "We define the model graph for training with native TF layers, i.e., `tf.nn.embedding_lookup` and `tf.keras.layers.Dense`. Besides, the embedding weights are stored in `tf.Variable`. We can then train the model and extract the trained weights of the embedding table. As for the dense layers, they are saved as a separate model graph, which can be loaded directly during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19779cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainModel(tf.keras.models.Model):\n",
    "    def __init__(self,\n",
    "                 init_tensors,\n",
    "                 slot_num,\n",
    "                 embed_vec_size,\n",
    "                 dense_dim,\n",
    "                 **kwargs):\n",
    "        super(TrainModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.slot_num = slot_num\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.dense_dim = dense_dim\n",
    "        self.init_tensors = init_tensors\n",
    "        self.params = tf.Variable(initial_value=tf.concat(self.init_tensors, axis=0))\n",
    "        self.concat = tf.keras.layers.Concatenate(axis=1, name=\"concatenate\")\n",
    "        self.fc_1 = tf.keras.layers.Dense(units=256, activation=None,\n",
    "                                                 kernel_initializer=\"ones\",\n",
    "                                                 bias_initializer=\"zeros\",\n",
    "                                                 name='fc_1')\n",
    "        self.fc_2 = tf.keras.layers.Dense(units=1, activation=None,\n",
    "                                                 kernel_initializer=\"ones\",\n",
    "                                                 bias_initializer=\"zeros\",\n",
    "                                                 name='fc_2')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        keys, dense_features = inputs[0], inputs[1]\n",
    "        embedding_vector = tf.nn.embedding_lookup(params=self.params, ids=keys)\n",
    "        embedding_vector = tf.reshape(embedding_vector, shape=[-1, self.slot_num * self.embed_vec_size])\n",
    "        concated_features = self.concat([embedding_vector, dense_features])\n",
    "        logit = self.fc_2(self.fc_1(concated_features))\n",
    "        return logit\n",
    "\n",
    "    def summary(self):\n",
    "        inputs = [tf.keras.Input(shape=(self.slot_num, ), dtype=args[\"tf_key_type\"]),\n",
    "                  tf.keras.Input(shape=(self.dense_dim, ), dtype=tf.float32)]\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8231b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    init_tensors = np.ones(shape=[args[\"max_vocabulary_size\"], args[\"embed_vec_size\"]], dtype=args[\"np_vector_type\"])\n",
    "    \n",
    "    model = TrainModel(init_tensors, args[\"slot_num\"], args[\"embed_vec_size\"], args[\"dense_dim\"])\n",
    "    model.summary()\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "    \n",
    "    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    def _train_step(inputs, labels):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logit = model(inputs)\n",
    "            loss = loss_fn(labels, logit)\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        return logit, loss\n",
    "\n",
    "    keys, dense_features, labels = generate_random_samples(args[\"global_batch_size\"]  * args[\"iter_num\"], args[\"vocabulary_range_per_slot\"], args[\"dense_dim\"])\n",
    "    dataset = tf_dataset(keys, dense_features, labels, args[\"global_batch_size\"])\n",
    "    for i, (keys, dense_features, labels) in enumerate(dataset):\n",
    "        inputs = [keys, dense_features]\n",
    "        _, loss = _train_step(inputs, labels)\n",
    "        print(\"-\"*20, \"Step {}, loss: {}\".format(i, loss),  \"-\"*20)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c6f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 02:41:00.863222: I tensorflow/core/platform/cpu_feature_guard.cc:152] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-31 02:41:01.391912: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30999 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.compat.v1.nn.embedding_lookup), but are not present in its tracked objects:   <tf.Variable 'Variable:0' shape=(50000, 16) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " tf.compat.v1.nn.embedding_look  (None, 5, 16)       0           ['input_1[0][0]']                \n",
      " up (TFOpLambda)                                                                                  \n",
      "                                                                                                  \n",
      " tf.reshape (TFOpLambda)        (None, 80)           0           ['tf.compat.v1.nn.embedding_looku\n",
      "                                                                 p[0][0]']                        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 90)           0           ['tf.reshape[0][0]',             \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " fc_1 (Dense)                   (None, 256)          23296       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " fc_2 (Dense)                   (None, 1)            257         ['fc_1[0][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,553\n",
      "Trainable params: 23,553\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "-------------------- Step 0, loss: 11092.1826171875 --------------------\n",
      "-------------------- Step 1, loss: 8587.974609375 --------------------\n",
      "-------------------- Step 2, loss: 6780.404296875 --------------------\n",
      "-------------------- Step 3, loss: 5393.8896484375 --------------------\n",
      "-------------------- Step 4, loss: 4023.296142578125 --------------------\n",
      "-------------------- Step 5, loss: 2579.7099609375 --------------------\n",
      "-------------------- Step 6, loss: 1797.363037109375 --------------------\n",
      "-------------------- Step 7, loss: 1062.6259765625 --------------------\n",
      "-------------------- Step 8, loss: 566.6324462890625 --------------------\n",
      "-------------------- Step 9, loss: 221.4973602294922 --------------------\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 80)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 90)           0           ['input_3[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " fc_1 (Dense)                   (None, 256)          23296       ['concatenate[1][0]']            \n",
      "                                                                                                  \n",
      " fc_2 (Dense)                   (None, 1)            257         ['fc_1[1][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,553\n",
      "Trainable params: 23,553\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 02:41:02.894935: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "WARNING:absl:Function `_wrapped_model` contains input name(s) args_0 with unsupported characters which will be renamed to args_0_1 in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: hps_tf_triton_dense.model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: hps_tf_triton_dense.model/assets\n"
     ]
    }
   ],
   "source": [
    "trained_model = train(args)\n",
    "weights_list = trained_model.get_weights()\n",
    "embedding_weights = weights_list[-1]\n",
    "dense_model = tf.keras.Model(trained_model.get_layer(\"concatenate\").input,\n",
    "                             trained_model.get_layer(\"fc_2\").output)\n",
    "dense_model.summary()\n",
    "dense_model.save(args[\"dense_model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0aaa551",
   "metadata": {},
   "source": [
    "## Create the inference graph with HPS LookupLayer\n",
    "In order to use HPS in the inference stage, we need to create a inference model graph which is almost the same as the train graph except that `tf.nn.embedding_lookup` is replaced by `hps.LookupLayer`. The trained dense model graph can be loaded directly, while the embedding weights should be converted to the formats required by HPS. \n",
    "\n",
    "We can then save the inference model graph, which will be ready to be loaded for inference deployment. Please note that the inference SavedModel that leverages HPS will be deployed with the Triton TensorFlow backend, thus implicit initialization of HPS should be enabled by specifying `ps_config_file` and `global_batch_size` in the constructor of  `hps.LookupLayer`. For more information, please refer to [HPS Initialize](https://nvidia-merlin.github.io/HugeCTR/master/hierarchical_parameter_server/api/initialize.html).\n",
    "\n",
    "To this end, we need to create a JSON configuration file and specify the details of the embedding tables for the models to be deployed. We only show how to deploy a model that has one embedding table here, and it can support multiple models with multiple embedding tables actually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5bff930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hps_tf_triton.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile hps_tf_triton.json\n",
    "{\n",
    "    \"supportlonglong\": true,\n",
    "    \"models\": [{\n",
    "        \"model\": \"hps_tf_triton\",\n",
    "        \"sparse_files\": [\"/hugectr/hierarchical_parameter_server/notebooks/model_repo/hps_tf_triton/1/hps_tf_triton_sparse_0.model\"],\n",
    "        \"num_of_worker_buffer_in_pool\": 3,\n",
    "        \"embedding_table_names\":[\"sparse_embedding1\"],\n",
    "        \"embedding_vecsize_per_table\": [16],\n",
    "        \"maxnum_catfeature_query_per_table_per_sample\": [5],\n",
    "        \"default_value_for_each_table\": [1.0],\n",
    "        \"deployed_device_list\": [0],\n",
    "        \"max_batch_size\": 1024,\n",
    "        \"cache_refresh_percentage_per_iteration\": 0.2,\n",
    "        \"hit_rate_threshold\": 1.0,\n",
    "        \"gpucacheper\": 1.0,\n",
    "        \"gpucache\": true\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb527dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_model_repo = \"/hugectr/hierarchical_parameter_server/notebooks/model_repo/hps_tf_triton/\"\n",
    "\n",
    "class InferenceModel(tf.keras.models.Model):\n",
    "    def __init__(self,\n",
    "                 slot_num,\n",
    "                 embed_vec_size,\n",
    "                 dense_dim,\n",
    "                 dense_model_path,\n",
    "                 **kwargs):\n",
    "        super(InferenceModel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.slot_num = slot_num\n",
    "        self.embed_vec_size = embed_vec_size\n",
    "        self.dense_dim = dense_dim\n",
    "        self.lookup_layer = hps.LookupLayer(model_name = \"hps_tf_triton\", \n",
    "                                            table_id = 0,\n",
    "                                            emb_vec_size = self.embed_vec_size,\n",
    "                                            emb_vec_dtype = args[\"tf_vector_type\"],\n",
    "                                            ps_config_file = triton_model_repo + args[\"ps_config_file\"],\n",
    "                                            global_batch_size = args[\"global_batch_size\"],\n",
    "                                            name = \"lookup\")\n",
    "        self.dense_model = tf.keras.models.load_model(dense_model_path)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        keys, dense_features = inputs[0], inputs[1]\n",
    "        embedding_vector = self.lookup_layer(keys)\n",
    "        embedding_vector = tf.reshape(embedding_vector, shape=[-1, self.slot_num * self.embed_vec_size])\n",
    "        logit = self.dense_model([embedding_vector, dense_features])\n",
    "        return logit\n",
    "\n",
    "    def summary(self):\n",
    "        inputs = [tf.keras.Input(shape=(self.slot_num, ), dtype=args[\"tf_key_type\"]),\n",
    "                  tf.keras.Input(shape=(self.dense_dim, ), dtype=tf.float32)]\n",
    "        model = tf.keras.models.Model(inputs=inputs, outputs=self.call(inputs))\n",
    "        return model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde93765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_inference_graph(args): \n",
    "    model = InferenceModel(args[\"slot_num\"], args[\"embed_vec_size\"], args[\"dense_dim\"], args[\"dense_model_path\"])\n",
    "    model.summary()\n",
    "    _ = model([tf.keras.Input(shape=(args[\"slot_num\"], ), dtype=args[\"tf_key_type\"]),\n",
    "               tf.keras.Input(shape=(args[\"dense_dim\"], ), dtype=tf.float32)])\n",
    "    model.save(args[\"saved_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e95633b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_sparse_model(embeddings_weights, embedding_table_path, embedding_vec_size):\n",
    "    os.system(\"mkdir -p {}\".format(embedding_table_path))\n",
    "    with open(\"{}/key\".format(embedding_table_path), 'wb') as key_file, \\\n",
    "        open(\"{}/emb_vector\".format(embedding_table_path), 'wb') as vec_file:\n",
    "      for key in range(embeddings_weights.shape[0]):\n",
    "        vec = embeddings_weights[key]\n",
    "        key_struct = struct.pack('q', key)\n",
    "        vec_struct = struct.pack(str(embedding_vec_size) + \"f\", *vec)\n",
    "        key_file.write(key_struct)\n",
    "        vec_file.write(vec_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bcbf9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " lookup (LookupLayer)           (None, 5, 16)        0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " tf.reshape_1 (TFOpLambda)      (None, 80)           0           ['lookup[0][0]']                 \n",
      "                                                                                                  \n",
      " input_5 (InputLayer)           [(None, 10)]         0           []                               \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, 1)            23553       ['tf.reshape_1[0][0]',           \n",
      "                                                                  'input_5[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,553\n",
      "Trainable params: 23,553\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "INFO:tensorflow:Assets written to: hps_tf_triton_tf_saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: hps_tf_triton_tf_saved_model/assets\n"
     ]
    }
   ],
   "source": [
    "convert_to_sparse_model(embedding_weights, args[\"embedding_table_path\"], args[\"embed_vec_size\"])\n",
    "create_and_save_inference_graph(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66ed354",
   "metadata": {},
   "source": [
    "## Deploy SavedModel using HPS with Triton TensorFlow Backend\n",
    "\n",
    "In order to deploy the inference SavedModel with the Triton TensorFlow backend, we need to create the model repository and define the `config.pbtxt` first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4823c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p model_repo/hps_tf_triton/1\n",
    "!mv hps_tf_triton_tf_saved_model model_repo/hps_tf_triton/1/model.savedmodel\n",
    "!mv hps_tf_triton_sparse_0.model model_repo/hps_tf_triton/1\n",
    "!mv hps_tf_triton.json model_repo/hps_tf_triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd41ec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model_repo/hps_tf_triton/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%writefile model_repo/hps_tf_triton/config.pbtxt\n",
    "name: \"hps_tf_triton\"\n",
    "platform: \"tensorflow_savedmodel\"\n",
    "max_batch_size:1024\n",
    "input [\n",
    "  {\n",
    "    name: \"input_1\"\n",
    "    data_type: TYPE_INT64\n",
    "    dims: [5]\n",
    "  },\n",
    "  {\n",
    "    name: \"input_2\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [10]\n",
    "  }\n",
    "]\n",
    "output [\n",
    "  {\n",
    "    name: \"output_1\"\n",
    "    data_type: TYPE_FP32\n",
    "    dims: [1]\n",
    "  }\n",
    "]\n",
    "version_policy: {\n",
    "        specific:{versions: 1}\n",
    "},\n",
    "instance_group [\n",
    "  {\n",
    "    count: 1\n",
    "    kind : KIND_GPU\n",
    "    gpus: [0]\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "945efc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mmodel_repo/hps_tf_triton\u001b[00m\r\n",
      "├── \u001b[01;34m1\u001b[00m\r\n",
      "│   ├── \u001b[01;34mhps_tf_triton_sparse_0.model\u001b[00m\r\n",
      "│   │   ├── emb_vector\r\n",
      "│   │   └── key\r\n",
      "│   └── \u001b[01;34mmodel.savedmodel\u001b[00m\r\n",
      "│       ├── \u001b[01;34massets\u001b[00m\r\n",
      "│       ├── keras_metadata.pb\r\n",
      "│       ├── saved_model.pb\r\n",
      "│       └── \u001b[01;34mvariables\u001b[00m\r\n",
      "│           ├── variables.data-00000-of-00001\r\n",
      "│           └── variables.index\r\n",
      "├── config.pbtxt\r\n",
      "└── hps_tf_triton.json\r\n",
      "\r\n",
      "5 directories, 8 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree model_repo/hps_tf_triton"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebb10e4",
   "metadata": {},
   "source": [
    "We can then launch the Triton inference server using the TensorFlow backend. Please note that `LD_PRELOAD` is utilized to load the custom TensorFlow operations (i.e., HPS related operations) into Triton. For more information, please refer to [TensorFlow Custom Operations in Triton](https://github.com/triton-inference-server/server/blob/main/docs/custom_operations.md#tensorflow).\n",
    "\n",
    "Note: `Since Background processes not supported by Jupyter, please launch the Triton Server according to the following command independently in the background`.\n",
    "\n",
    "> **LD_PRELOAD=/usr/local/lib/python3.8/dist-packages/merlin_hps-1.0.0-py3.8-linux-x86_64.egg/hierarchical_parameter_server/lib/libhierarchical_parameter_server.so tritonserver --model-repository=/hugectr/hierarchical_parameter_server/notebooks/model_repo --backend-config=tensorflow,version=2 --load-model=hps_tf_triton --model-control-mode=explicit**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462cb59e",
   "metadata": {},
   "source": [
    "We can then send the requests to the Triton inference server using the HTTP client. Please note that HPS will be initialized implicitly when the first request is processed at the server side, and the latency can be higher than that of later requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e482a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GET /v2/health/live, headers None\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-length': '0', 'content-type': 'text/plain'}>\n",
      "POST /v2/repository/index, headers None\n",
      "\n",
      "<HTTPSocketPoolResponse status=200 headers={'content-type': 'application/json', 'content-length': '56'}>\n",
      "bytearray(b'[{\"name\":\"hps_tf_triton\",\"version\":\"1\",\"state\":\"READY\"}]')\n",
      "--------------------------Request 0--------------------------\n",
      "Prediction result:\n",
      "[[102.5896  ]\n",
      " [109.57728 ]\n",
      " [105.14982 ]\n",
      " [112.536064]\n",
      " [115.500206]\n",
      " [122.54994 ]\n",
      " [118.42626 ]\n",
      " [116.46372 ]\n",
      " [109.31565 ]\n",
      " [123.4528  ]\n",
      " [106.171906]\n",
      " [108.97251 ]\n",
      " [ 92.7147  ]\n",
      " [ 92.135   ]\n",
      " [103.82919 ]\n",
      " [119.42267 ]]\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '1', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [16, 1], 'parameters': {'binary_data_size': 64}}]}\n",
      "--------------------------Request 1--------------------------\n",
      "Prediction result:\n",
      "[[108.39017 ]\n",
      " [111.06535 ]\n",
      " [112.19471 ]\n",
      " [123.16409 ]\n",
      " [ 84.799545]\n",
      " [115.29873 ]\n",
      " [104.93053 ]\n",
      " [ 92.51399 ]\n",
      " [111.03866 ]\n",
      " [118.73721 ]\n",
      " [119.996704]\n",
      " [111.64917 ]\n",
      " [111.96098 ]\n",
      " [106.95992 ]\n",
      " [118.8165  ]\n",
      " [102.89214 ]]\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '1', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [16, 1], 'parameters': {'binary_data_size': 64}}]}\n",
      "--------------------------Request 2--------------------------\n",
      "Prediction result:\n",
      "[[115.43975 ]\n",
      " [104.05401 ]\n",
      " [ 95.1138  ]\n",
      " [109.50248 ]\n",
      " [117.69166 ]\n",
      " [111.92008 ]\n",
      " [ 99.65907 ]\n",
      " [ 91.395035]\n",
      " [103.35495 ]\n",
      " [115.99719 ]\n",
      " [114.05845 ]\n",
      " [ 90.95559 ]\n",
      " [110.51797 ]\n",
      " [105.39578 ]\n",
      " [104.69898 ]\n",
      " [ 97.37328 ]]\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '1', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [16, 1], 'parameters': {'binary_data_size': 64}}]}\n",
      "--------------------------Request 3--------------------------\n",
      "Prediction result:\n",
      "[[119.3963  ]\n",
      " [125.18664 ]\n",
      " [123.5703  ]\n",
      " [112.66611 ]\n",
      " [ 99.078514]\n",
      " [105.94452 ]\n",
      " [102.65439 ]\n",
      " [111.734314]\n",
      " [ 91.28878 ]\n",
      " [104.32374 ]\n",
      " [117.849236]\n",
      " [102.520256]\n",
      " [115.76198 ]\n",
      " [102.74941 ]\n",
      " [101.43743 ]\n",
      " [115.935295]]\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '1', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [16, 1], 'parameters': {'binary_data_size': 64}}]}\n",
      "--------------------------Request 4--------------------------\n",
      "Prediction result:\n",
      "[[111.10583 ]\n",
      " [107.11977 ]\n",
      " [ 93.52705 ]\n",
      " [119.05332 ]\n",
      " [116.221054]\n",
      " [101.29974 ]\n",
      " [111.83873 ]\n",
      " [106.383804]\n",
      " [ 95.079666]\n",
      " [118.63491 ]\n",
      " [101.41594 ]\n",
      " [107.80403 ]\n",
      " [113.61658 ]\n",
      " [104.66516 ]\n",
      " [107.94727 ]\n",
      " [106.81342 ]]\n",
      "Response details:\n",
      "{'model_name': 'hps_tf_triton', 'model_version': '1', 'outputs': [{'name': 'output_1', 'datatype': 'FP32', 'shape': [16, 1], 'parameters': {'binary_data_size': 64}}]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_gpu = 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, range(num_gpu)))\n",
    "\n",
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "from tritonclient.utils import *\n",
    "\n",
    "triton_client = httpclient.InferenceServerClient(url=\"localhost:8000\", verbose=True)\n",
    "triton_client.is_server_live()\n",
    "triton_client.get_model_repository_index()\n",
    "\n",
    "num_requests = 5\n",
    "num_samples = 16\n",
    "\n",
    "for i in range(num_requests):\n",
    "    print(\"--------------------------Request {}--------------------------\".format(i))\n",
    "    key_tensor, dense_tensor, _ = generate_random_samples(num_samples, args[\"vocabulary_range_per_slot\"], args[\"dense_dim\"])\n",
    "\n",
    "    inputs = [\n",
    "        httpclient.InferInput(\"input_1\", \n",
    "                              key_tensor.shape,\n",
    "                              np_to_triton_dtype(np.int64)),\n",
    "        httpclient.InferInput(\"input_2\", \n",
    "                              dense_tensor.shape,\n",
    "                              np_to_triton_dtype(np.float32)),\n",
    "    ]\n",
    "\n",
    "    inputs[0].set_data_from_numpy(key_tensor)\n",
    "    inputs[1].set_data_from_numpy(dense_tensor)\n",
    "    outputs = [\n",
    "        httpclient.InferRequestedOutput(\"output_1\")\n",
    "    ]\n",
    "\n",
    "    # print(\"Input key tensor is \\n{}\".format(key_tensor))\n",
    "    # print(\"Input dense tensor is \\n{}\".format(dense_tensor))\n",
    "    model_name = \"hps_tf_triton\"\n",
    "    with httpclient.InferenceServerClient(\"localhost:8000\") as client:\n",
    "        response = client.infer(model_name,\n",
    "                                inputs,\n",
    "                                outputs=outputs)\n",
    "        result = response.get_response()\n",
    "\n",
    "        print(\"Prediction result:\\n{}\".format(response.as_numpy(\"output_1\")))\n",
    "        print(\"Response details:\\n{}\".format(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
