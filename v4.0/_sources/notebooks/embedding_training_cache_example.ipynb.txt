{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42245a1f",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/notebooks/dlsw-notebooks/merlin_hugectr_embedding-training-cache-example/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# Embedding Training Cache Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec20d41e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "[Embedding Training Cache](https://nvidia-merlin.github.io/HugeCTR/master/hugectr_embedding_training_cache.html) enables you to train huge models that cannot fit into GPU memory in one time. In this example, we will go through an end-to-end training procedure using the embedding training cache feature of HugeCTR. We are going to use the Criteo dataset as our data source and NVTabular as our data preprocessing tool. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cded89fa",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "-  [Installation](#installation)\n",
    "-  [Data Preparation](#data-preparation)\n",
    "-  [Extract keyset](#extract-keyset)\n",
    "-  [Training using HugeCTR](#training-using-hugectr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5b745",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "### Get HugeCTR from NVIDIA GPU Cloud\n",
    "\n",
    "The HugeCTR Python module is preinstalled in the 22.04 and later [Merlin Training Container](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/merlin/containers/merlin-training): `nvcr.io/nvidia/merlin/merlin-training:22.04`.\n",
    "\n",
    "You can check the existence of required libraries by running the following Python code after launching this container.\n",
    "\n",
    "```bash\n",
    "$ python3 -c \"import hugectr\"\n",
    "```\n",
    "\n",
    "**Note**: This Python module contains both training APIs and offline inference APIs. For online inference with Triton, please refer to [HugeCTR Backend](https://github.com/triton-inference-server/hugectr_backend).\n",
    "\n",
    "> If you prefer to build HugeCTR from the source code instead of using the NGC container, please refer to the\n",
    "> [How to Start Your Development](https://nvidia-merlin.github.io/HugeCTR/master/hugectr_contributor_guide.html#how-to-start-your-development)\n",
    "> documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcda737",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50f25d9",
   "metadata": {},
   "source": [
    "First, make a folder to store our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70b82e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir etc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401b46db",
   "metadata": {},
   "source": [
    "Second, make a script that uses the [HugeCTR Data Generator](https://nvidia-merlin.github.io/HugeCTR/master/hugectr_user_guide.html#generating-synthetic-data-and-benchmarks) to generate datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19817f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generate_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generate_data.py\n",
    "\n",
    "import hugectr\n",
    "from hugectr.tools import DataGenerator, DataGeneratorParams\n",
    "from mpi4py import MPI\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=(\"Data Generation\"))\n",
    "\n",
    "parser.add_argument(\"--num_files\", type=int, help=\"number of files in training data\", default = 8)\n",
    "parser.add_argument(\"--eval_num_files\", type=int, help=\"number of files in validation data\", default = 2)\n",
    "parser.add_argument('--num_samples_per_file', type=int, help=\"number of samples per file\", default=1000000)\n",
    "parser.add_argument('--dir_name', type=str, help=\"data directory name(Required)\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_generator_params = DataGeneratorParams(\n",
    "  format = hugectr.DataReaderType_t.Parquet,\n",
    "  label_dim = 1,\n",
    "  dense_dim = 13,\n",
    "  num_slot = 26,\n",
    "  num_files = args.num_files,\n",
    "  eval_num_files = args.eval_num_files,\n",
    "  i64_input_key = True,\n",
    "  num_samples_per_file = args.num_samples_per_file,\n",
    "  source = \"./etc_data/\" + args.dir_name + \"/file_list.txt\",\n",
    "  eval_source = \"./etc_data/\" + args.dir_name + \"/file_list_test.txt\",\n",
    "  slot_size_array = [12988, 7129, 8720, 5820, 15196, 4, 4914, 1020, 30, 14274, 10220, 15088, 10, 1518, 3672, 48, 4, 820, 15, 12817, 13908, 13447, 9447, 5867, 45, 33],\n",
    "  # for parquet, check_type doesn't make any difference\n",
    "  check_type = hugectr.Check_t.Non,\n",
    "  dist_type = hugectr.Distribution_t.PowerLaw,\n",
    "  power_law_type = hugectr.PowerLaw_t.Short)\n",
    "data_generator = DataGenerator(data_generator_params)\n",
    "data_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f03740eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][09:00:01][INFO][RK0][main]: Generate Parquet dataset\n",
      "[HCTR][09:00:01][INFO][RK0][main]: train data folder: ./etc_data/file0, eval data folder: ./etc_data/file0, slot_size_array: 12988, 7129, 8720, 5820, 15196, 4, 4914, 1020, 30, 14274, 10220, 15088, 10, 1518, 3672, 48, 4, 820, 15, 12817, 13908, 13447, 9447, 5867, 45, 33, nnz array: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, #files for train: 8, #files for eval: 2, #samples per file: 1000000, Use power law distribution: 1, alpha of power law: 1.3\n",
      "[HCTR][09:00:01][INFO][RK0][main]: ./etc_data/file0 exist\n",
      "[HCTR][09:00:01][INFO][RK0][main]: ./etc_data/file0/train/gen_0.parquet\n",
      "[HCTR][09:00:05][INFO][RK0][main]: ./etc_data/file0/train/gen_1.parquet\n",
      "[HCTR][09:00:08][INFO][RK0][main]: ./etc_data/file0/train/gen_2.parquet\n",
      "[HCTR][09:00:11][INFO][RK0][main]: ./etc_data/file0/train/gen_3.parquet\n",
      "[HCTR][09:00:14][INFO][RK0][main]: ./etc_data/file0/train/gen_4.parquet\n",
      "[HCTR][09:00:17][INFO][RK0][main]: ./etc_data/file0/train/gen_5.parquet\n",
      "[HCTR][09:00:20][INFO][RK0][main]: ./etc_data/file0/train/gen_6.parquet\n",
      "[HCTR][09:00:23][INFO][RK0][main]: ./etc_data/file0/train/gen_7.parquet\n",
      "[HCTR][09:00:26][INFO][RK0][main]: ./etc_data/file0/file_list.txt done!\n",
      "[HCTR][09:00:26][INFO][RK0][main]: ./etc_data/file0/val/gen_0.parquet\n",
      "[HCTR][09:00:29][INFO][RK0][main]: ./etc_data/file0/val/gen_1.parquet\n",
      "[HCTR][09:00:32][INFO][RK0][main]: ./etc_data/file0/file_list_test.txt done!\n"
     ]
    }
   ],
   "source": [
    "!python generate_data.py --dir_name \"file0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f789c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][09:01:09][INFO][RK0][main]: Generate Parquet dataset\n",
      "[HCTR][09:01:09][INFO][RK0][main]: train data folder: ./etc_data/file1, eval data folder: ./etc_data/file1, slot_size_array: 12988, 7129, 8720, 5820, 15196, 4, 4914, 1020, 30, 14274, 10220, 15088, 10, 1518, 3672, 48, 4, 820, 15, 12817, 13908, 13447, 9447, 5867, 45, 33, nnz array: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, #files for train: 8, #files for eval: 2, #samples per file: 1000000, Use power law distribution: 1, alpha of power law: 1.3\n",
      "[HCTR][09:01:09][INFO][RK0][main]: ./etc_data/file1 exist\n",
      "[HCTR][09:01:09][INFO][RK0][main]: ./etc_data/file1/train/gen_0.parquet\n",
      "[HCTR][09:01:13][INFO][RK0][main]: ./etc_data/file1/train/gen_1.parquet\n",
      "[HCTR][09:01:16][INFO][RK0][main]: ./etc_data/file1/train/gen_2.parquet\n",
      "[HCTR][09:01:19][INFO][RK0][main]: ./etc_data/file1/train/gen_3.parquet\n",
      "[HCTR][09:01:22][INFO][RK0][main]: ./etc_data/file1/train/gen_4.parquet\n",
      "[HCTR][09:01:26][INFO][RK0][main]: ./etc_data/file1/train/gen_5.parquet\n",
      "[HCTR][09:01:29][INFO][RK0][main]: ./etc_data/file1/train/gen_6.parquet\n",
      "[HCTR][09:01:32][INFO][RK0][main]: ./etc_data/file1/train/gen_7.parquet\n",
      "[HCTR][09:01:35][INFO][RK0][main]: ./etc_data/file1/file_list.txt done!\n",
      "[HCTR][09:01:35][INFO][RK0][main]: ./etc_data/file1/val/gen_0.parquet\n",
      "[HCTR][09:01:38][INFO][RK0][main]: ./etc_data/file1/val/gen_1.parquet\n",
      "[HCTR][09:01:41][INFO][RK0][main]: ./etc_data/file1/file_list_test.txt done!\n"
     ]
    }
   ],
   "source": [
    "!python generate_data.py --dir_name \"file1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58113c1d",
   "metadata": {},
   "source": [
    "## Extract Keyset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b55e37",
   "metadata": {},
   "source": [
    "The HugeCTR repository on GitHub includes a keyset generator script for Parquet datasets. See the `generate_keyset.py` file in the [keyset_scripts](https://github.com/NVIDIA-Merlin/HugeCTR/tree/master/tools/keyset_scripts) directory of the repository. We can use the script to generate keyset for our training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c61570f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-06 09:01:54,758 Extracted keyset from ./etc_data/file0/train\n"
     ]
    }
   ],
   "source": [
    "!python generate_keyset.py --src_dir_path ./etc_data/file0/train --keyset_path ./etc_data/file0/train/_hugectr.keyset  --slot_size_array 12988 7129 8720 5820 15196 4 4914 1020 30 14274 10220 15088 10 1518 3672 48 4 820 15 12817 13908 13447 9447 5867 45 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cff6ae",
   "metadata": {},
   "source": [
    "Do the same thing for file2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cff2cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-06 09:02:01,163 Extracted keyset from ./etc_data/file1/train\n"
     ]
    }
   ],
   "source": [
    "!python generate_keyset.py --src_dir_path ./etc_data/file1/train --keyset_path ./etc_data/file1/train/_hugectr.keyset  --slot_size_array 12988 7129 8720 5820 15196 4 4914 1020 30 14274 10220 15088 10 1518 3672 48 4 820 15 12817 13908 13447 9447 5867 45 33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab2d2dc",
   "metadata": {},
   "source": [
    "Run `ls -l ./data` to make sure we have data and keyset ready:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9d5a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 801387\n",
      "-rw-r--r-- 1 root dip  1256424 Jun  6 09:00 _hugectr.keyset\n",
      "-rw-r--r-- 1 root dip     1959 Jun  6 08:58 _metadata.json\n",
      "-rw-r--r-- 1 root dip 91956719 Jun  6 08:58 gen_0.parquet\n",
      "-rw-r--r-- 1 root dip 91951983 Jun  6 08:58 gen_1.parquet\n",
      "-rw-r--r-- 1 root dip 91956559 Jun  6 08:58 gen_2.parquet\n",
      "-rw-r--r-- 1 root dip 91954535 Jun  6 08:58 gen_3.parquet\n",
      "-rw-r--r-- 1 root dip 91951501 Jun  6 08:58 gen_4.parquet\n",
      "-rw-r--r-- 1 root dip 91963545 Jun  6 08:58 gen_5.parquet\n",
      "-rw-r--r-- 1 root dip 91961051 Jun  6 08:58 gen_6.parquet\n",
      "-rw-r--r-- 1 root dip 91955276 Jun  6 08:58 gen_7.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./etc_data/file0/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70fb1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 801387\n",
      "-rw-r--r-- 1 root dip  1256432 Jun  6 09:00 _hugectr.keyset\n",
      "-rw-r--r-- 1 root dip     1959 Jun  6 08:59 _metadata.json\n",
      "-rw-r--r-- 1 root dip 91959333 Jun  6 08:59 gen_0.parquet\n",
      "-rw-r--r-- 1 root dip 91962190 Jun  6 08:59 gen_1.parquet\n",
      "-rw-r--r-- 1 root dip 91960276 Jun  6 08:59 gen_2.parquet\n",
      "-rw-r--r-- 1 root dip 91951335 Jun  6 08:59 gen_3.parquet\n",
      "-rw-r--r-- 1 root dip 91957041 Jun  6 08:59 gen_4.parquet\n",
      "-rw-r--r-- 1 root dip 91959877 Jun  6 08:59 gen_5.parquet\n",
      "-rw-r--r-- 1 root dip 91975033 Jun  6 08:59 gen_6.parquet\n",
      "-rw-r--r-- 1 root dip 91962975 Jun  6 08:59 gen_7.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls -l ./etc_data/file1/train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f5701d",
   "metadata": {},
   "source": [
    "## Training using HugeCTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e25db8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting etc_sample.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile etc_sample.py\n",
    "import hugectr\n",
    "from mpi4py import MPI\n",
    "solver = hugectr.CreateSolver(max_eval_batches = 5000,\n",
    "                              batchsize_eval = 1024,\n",
    "                              batchsize = 1024,\n",
    "                              lr = 0.001,\n",
    "                              vvgpu = [[0]],\n",
    "                              i64_input_key = True,\n",
    "                              use_mixed_precision = False,\n",
    "                              repeat_dataset = False)\n",
    "reader = hugectr.DataReaderParams(data_reader_type = hugectr.DataReaderType_t.Parquet,\n",
    "                          source = [\"./etc_data/file0/file_list.txt\"],\n",
    "                          keyset = [\"./etc_data/file0/train/_hugectr.keyset\"],\n",
    "                          eval_source = \"./etc_data/file0/file_list_test.txt\",\n",
    "                          slot_size_array = [12988, 7129, 8720, 5820, 15196, 4, 4914, 1020, 30, 14274, 10220, 15088, 10, 1518, 3672, 48, 4, 820, 15, 12817, 13908, 13447, 9447, 5867, 45, 33],\n",
    "                          check_type = hugectr.Check_t.Non)\n",
    "optimizer = hugectr.CreateOptimizer(optimizer_type = hugectr.Optimizer_t.Adam)\n",
    "hc_cnfg = hugectr.CreateHMemCache(num_blocks = 1, target_hit_rate = 0.5, max_num_evict = 0)\n",
    "etc = hugectr.CreateETC(ps_types = [hugectr.TrainPSType_t.Cached],\n",
    "                       sparse_models = [\"./dcn_sparse_model\"],\n",
    "                       local_paths = [\"./\"], hmem_cache_configs = [hc_cnfg])\n",
    "model = hugectr.Model(solver, reader, optimizer, etc)\n",
    "model.add(hugectr.Input(label_dim = 1, label_name = \"label\",\n",
    "                        dense_dim = 13, dense_name = \"dense\",\n",
    "                        data_reader_sparse_param_array = \n",
    "                        [hugectr.DataReaderSparseParam(\"data1\", 1, True, 26)]))\n",
    "model.add(hugectr.SparseEmbedding(embedding_type = hugectr.Embedding_t.DistributedSlotSparseEmbeddingHash, \n",
    "                            workspace_size_per_gpu_in_mb = 5000,\n",
    "                            embedding_vec_size = 16,\n",
    "                            combiner = \"sum\",\n",
    "                            sparse_embedding_name = \"sparse_embedding1\",\n",
    "                            bottom_name = \"data1\",\n",
    "                            optimizer = optimizer))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Reshape,\n",
    "                            bottom_names = [\"sparse_embedding1\"],\n",
    "                            top_names = [\"reshape1\"],\n",
    "                            leading_dim=416))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"reshape1\", \"dense\"], top_names = [\"concat1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.MultiCross,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"multicross1\"],\n",
    "                            num_layers=6))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat1\"],\n",
    "                            top_names = [\"fc1\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc1\"],\n",
    "                            top_names = [\"relu1\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu1\"],\n",
    "                            top_names = [\"dropout1\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"dropout1\"],\n",
    "                            top_names = [\"fc2\"],\n",
    "                            num_output=1024))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.ReLU,\n",
    "                            bottom_names = [\"fc2\"],\n",
    "                            top_names = [\"relu2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Dropout,\n",
    "                            bottom_names = [\"relu2\"],\n",
    "                            top_names = [\"dropout2\"],\n",
    "                            dropout_rate=0.5))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.Concat,\n",
    "                            bottom_names = [\"dropout2\", \"multicross1\"],\n",
    "                            top_names = [\"concat2\"]))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.InnerProduct,\n",
    "                            bottom_names = [\"concat2\"],\n",
    "                            top_names = [\"fc3\"],\n",
    "                            num_output=1))\n",
    "model.add(hugectr.DenseLayer(layer_type = hugectr.Layer_t.BinaryCrossEntropyLoss,\n",
    "                            bottom_names = [\"fc3\", \"label\"],\n",
    "                            top_names = [\"loss\"]))\n",
    "model.compile()\n",
    "model.summary()\n",
    "model.graph_to_json(graph_config_file = \"dcn.json\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "\n",
    "model.set_source(source = [\"etc_data/file1/file_list.txt\"], keyset = [\"etc_data/file1/train/_hugectr.keyset\"], eval_source = \"etc_data/file1/file_list_test.txt\")\n",
    "model.fit(num_epochs = 1, display = 500, eval_interval = 1000)\n",
    "\n",
    "model.save_params_to_files(\"dcn_etc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b606673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HCTR][09:02:26][INFO][RK0][main]: Empty embedding, trained table will be stored in ./dcn_sparse_model\n",
      "HugeCTR Version: 3.5\n",
      "====================================================Model Init=====================================================\n",
      "[HCTR][09:02:26][WARNING][RK0][main]: The model name is not specified when creating the solver.\n",
      "[HCTR][09:02:26][WARNING][RK0][main]: MPI was already initialized somewhere elese. Lifetime service disabled.\n",
      "[HCTR][09:02:26][INFO][RK0][main]: Global seed is 1968709516\n",
      "[HCTR][09:02:26][INFO][RK0][main]: Device to NUMA mapping:\n",
      "  GPU 0 ->  node 0\n",
      "[HCTR][09:02:27][WARNING][RK0][main]: Peer-to-peer access cannot be fully enabled.\n",
      "[HCTR][09:02:27][INFO][RK0][main]: Start all2all warmup\n",
      "[HCTR][09:02:27][INFO][RK0][main]: End all2all warmup\n",
      "[HCTR][09:02:27][INFO][RK0][main]: Using All-reduce algorithm: NCCL\n",
      "[HCTR][09:02:27][INFO][RK0][main]: Device 0: Tesla V100-SXM2-32GB\n",
      "[HCTR][09:02:27][INFO][RK0][main]: num of DataReader workers: 1\n",
      "[HCTR][09:02:27][INFO][RK0][main]: Vocabulary size: 157054\n",
      "[HCTR][09:02:27][INFO][RK0][main]: max_vocabulary_size_per_gpu_=27306666\n",
      "[HCTR][09:02:27][INFO][RK0][main]: Graph analysis to resolve tensor dependency\n",
      "[HCTR][09:02:27][INFO][RK0][main]: Add Slice layer for tensor: concat1, creating 2 copies\n",
      "===================================================Model Compile===================================================\n",
      "[HCTR][09:02:31][INFO][RK0][main]: gpu0 start to init embedding\n",
      "[HCTR][09:02:31][INFO][RK0][main]: gpu0 init embedding done\n",
      "[HCTR][09:02:31][INFO][RK0][main]: Enable HMemCache-Based Parameter Server\n",
      "[HCTR][09:02:31][INFO][RK0][main]: ./dcn_sparse_model/key doesn't exist, created\n",
      "[HCTR][09:02:31][INFO][RK0][main]: ./dcn_sparse_model/emb_vector doesn't exist, created\n",
      "[HCTR][09:02:31][INFO][RK0][main]: ./dcn_sparse_model/Adam.m doesn't exist, created\n",
      "[HCTR][09:02:31][INFO][RK0][main]: ./dcn_sparse_model/Adam.v doesn't exist, created\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Starting AUC NCCL warm-up\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Warm-up done\n",
      "===================================================Model Summary===================================================\n",
      "[HCTR][09:02:36][INFO][RK0][main]: label                                   Dense                         Sparse                        \n",
      "label                                   dense                          data1                         \n",
      "(None, 1)                               (None, 13)                              \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "Layer Type                              Input Name                    Output Name                   Output Shape                  \n",
      "——————————————————————————————————————————————————————————————————————————————————————————————————————————————————\n",
      "DistributedSlotSparseEmbeddingHash      data1                         sparse_embedding1             (None, 26, 16)                \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Reshape                                 sparse_embedding1             reshape1                      (None, 416)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  reshape1                      concat1                       (None, 429)                   \n",
      "                                        dense                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Slice                                   concat1                       concat1_slice0                (None, 429)                   \n",
      "                                                                      concat1_slice1                (None, 429)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "MultiCross                              concat1_slice0                multicross1                   (None, 429)                   \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat1_slice1                fc1                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc1                           relu1                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu1                         dropout1                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            dropout1                      fc2                           (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "ReLU                                    fc2                           relu2                         (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Dropout                                 relu2                         dropout2                      (None, 1024)                  \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "Concat                                  dropout2                      concat2                       (None, 1453)                  \n",
      "                                        multicross1                                                                               \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "InnerProduct                            concat2                       fc3                           (None, 1)                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "BinaryCrossEntropyLoss                  fc3                           loss                                                        \n",
      "                                        label                                                                                     \n",
      "------------------------------------------------------------------------------------------------------------------\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Save the model graph to dcn.json successfully\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Use embedding training cache mode with number of training sources: 1, number of epochs: 1\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][09:02:36][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][09:02:36][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Evaluation source file: ./etc_data/file0/file_list_test.txt\n",
      "[HCTR][09:02:36][INFO][RK0][main]: --------------------Epoch 0, source file: ./etc_data/file0/file_list.txt--------------------\n",
      "[HCTR][09:02:36][INFO][RK0][main]: Preparing embedding table for next pass\n",
      "[HCTR][09:02:36][INFO][RK0][main]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HCTR][09:02:47][INFO][RK0][main]: Iter: 500 Time(500 iters): 10.3413s Loss: 0.692548 lr:0.001\n",
      "[HCTR][09:02:56][INFO][RK0][main]: Iter: 1000 Time(500 iters): 9.39275s Loss: 0.692917 lr:0.001\n",
      "[HCTR][09:02:56][INFO][RK0][main]: eval drop incomplete batch. batchsize:168\n",
      "[HCTR][09:02:56][INFO][RK0][main]: Evaluation, AUC: 0.499922\n",
      "[HCTR][09:02:56][INFO][RK0][main]: Eval Time for 5000 iters: 0.280399s\n",
      "[HCTR][09:03:06][INFO][RK0][main]: Iter: 1500 Time(500 iters): 9.65627s Loss: 0.693724 lr:0.001\n",
      "[HCTR][09:03:14][INFO][RK0][main]: train drop incomplete batch. batchsize:672\n",
      "=====================================================Model Fit=====================================================\n",
      "[HCTR][09:03:14][INFO][RK0][main]: Use embedding training cache mode with number of training sources: 1, number of epochs: 1\n",
      "[HCTR][09:03:14][INFO][RK0][main]: Training batchsize: 1024, evaluation batchsize: 1024\n",
      "[HCTR][09:03:14][INFO][RK0][main]: Evaluation interval: 1000, snapshot interval: 10000\n",
      "[HCTR][09:03:14][INFO][RK0][main]: Dense network trainable: True\n",
      "[HCTR][09:03:14][INFO][RK0][main]: Sparse embedding sparse_embedding1 trainable: True\n",
      "[HCTR][09:03:14][INFO][RK0][main]: Use mixed precision: False, scaler: 1.000000, use cuda graph: True\n",
      "[HCTR][09:03:14][INFO][RK0][main]: lr: 0.001000, warmup_steps: 1, end_lr: 0.000000\n",
      "[HCTR][09:03:14][INFO][RK0][main]: decay_start: 0, decay_steps: 1, decay_power: 2.000000\n",
      "[HCTR][09:03:14][INFO][RK0][main]: Evaluation source file: etc_data/file1/file_list_test.txt\n",
      "[HCTR][09:03:14][INFO][RK0][main]: --------------------Epoch 0, source file: etc_data/file1/file_list.txt--------------------\n",
      "[HCTR][09:03:14][INFO][RK0][main]: Preparing embedding table for next pass\n",
      "[HCTR][09:03:15][INFO][RK0][main]: HMEM-Cache PS: Hit rate [dump]: 0 %\n",
      "[HCTR][09:03:15][INFO][RK0][main]: HMEM-Cache PS: Hit rate [load]: 0 %\n",
      "[HCTR][09:03:15][INFO][RK0][main]: HMEM-Cache PS: Hit rate [dump]: 0 %\n",
      "[HCTR][09:03:15][INFO][RK0][main]: Sync blocks from HMEM-Cache to SSD\n",
      " \u001b[38;2;89;255;89m ████████████████████████████████████████▏ \u001b[1m\u001b[31m100.0% \u001b[34m[   1/   1 | 53.8 Hz | 0s<0s]  \u001b[0m\u001b[32m\u001b[0mm\n",
      "[HCTR][09:03:15][INFO][RK0][main]: Dumping dense weights to file, successful\n",
      "[HCTR][09:03:15][INFO][RK0][main]: Dumping dense optimizer states to file, successful\n"
     ]
    }
   ],
   "source": [
    "!python3 etc_sample.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
